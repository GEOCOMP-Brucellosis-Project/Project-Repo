{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bovine Brucellosis\n",
    "\n",
    "### Katharine Young, Tanner Johnson, Mohsen Ahmadkhani, Finn Roberts\n",
    "\n",
    "A spatio-temporal analysis of the relationship between human and animal Brucellosis cases, focusing on the association between animal vaccination practices and disease transmission by county in Iran.\n",
    "\n",
    "This project seeks to investigate the relationship between disease incidence in humans with various factors at the county level. We are considering animal vaccination status expressed as percent vaccinated out of total population in county, socioeconomic (SES) variables, and environmental factors including precipitation, temperature, NDVI, and elevation to see if they have any affect.\n",
    "\n",
    "## Workflow and Current Status\n",
    "1. **Collect animal, human data, and socioeconomic data <-- Completed**\n",
    "1. **Analyze data and develop project direction/scope <-- Completed**\n",
    "1. **Collect environmental data using Google Earth Engine <-- Completed**\n",
    "1. **Clean and merge all data <-- Completed**\n",
    "1. Investigate trends in human incidence through regression/multiple regression models <--In Progress\n",
    "\n",
    "Validating regression models is a crucial next step, though likely outside the scope of this semester.\n",
    "\n",
    "In this document you will find a working prototype of our project code so far. The code cleans the human, animal, SES, and environmental datasets, and merges them into one. It also investigates spatial clustering trends in the human and animal data through Moran's i. \n",
    "\n",
    "To run this notebook, place it in the \"sup_materials.zip\" folder (accompanying this submission) after decompressing and it should be prepared to read in the required data files properly.\n",
    "\n",
    "The code for summarizing each environmental variable by county is found in a separate jupyter notebook called \"EarthEngineCode_GoogleCollab.ipynb\". To run this code, either run the notebook in a Google Collab Notebook (preferred) or install the earthengine module into your local python environment. You will need an EarthEngine account to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning\n",
    "\n",
    "Our project requires integrating information from the following data sources:\n",
    "\n",
    "- Human data\n",
    "    - Data on human brucellosis cases in Iran from 2015 to 2018\n",
    "    - Contains demographic information on each case as well as information on past interaction with livestock, livestock vaccination status, and any known link to unpasteurized dairy consumption\n",
    "- Animal data\n",
    "    - Data on brucellosis incidence in livestock in Iran from 2009 to 2017\n",
    "    - Contains results from sampling of barns across the country (includes number of animals sampled, number infected, number healthy, and number of suspicious cases)\n",
    "- Socioeconomic status data\n",
    "    - Data containing average household size and socioeconomic status quintile at the province level\n",
    "- Population data\n",
    "    - Contains state-reported population data for each county\n",
    "- Environmental data\n",
    "    - NDVI, temperature, humidity, and precipitation data obtained using Google Earth Engine\n",
    "- Spatial data\n",
    "    - Shapefile of Iranian counties\n",
    "    \n",
    "In the code sections below, we read in each of these datasets in turn (environmental data is saved for later in the script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import Levenshtein as leven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File path\n",
    "fp = os.getcwd()\n",
    "\n",
    "## Read animal data and update columns\n",
    "animal_data = pd.read_csv(os.path.join(fp, 'Data','animal_vac_data.csv'))\n",
    "animal_data.columns = ['id', 'unitCode', 'unitType', 'province',\n",
    "                       'county', 'livestock_type', 'time_j', 'time_g',\n",
    "                       'lat' , 'long', 'n_sample', 'n_checked', 'n_infected', \n",
    "                       'n_rejected', 'n_suspicious']\n",
    "\n",
    "## Create new columns storing month and year of animal testing\n",
    "animal_data[['month', 'year']] = animal_data['time_g'].str.split('/', expand = True)[[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapefile data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in Iran shapefile data\n",
    "iran_data = gpd.read_file(os.path.join(fp, 'Iran_shp/iran_admin.shp'))\n",
    "\n",
    "## Subset to relevant columns and update names\n",
    "iran_data = iran_data[['ADM2_EN','ADM2_FA','ADM1_EN','ADM1_FA','Shape_Leng','Shape_Area','geometry']]\n",
    "iran_data.columns = ['county_en', 'county_fa', 'province_en', 'province_fa', 'shape_len', 'shape_area', 'geometry']\n",
    "\n",
    "## This accidental escape sequence is problematic later, so deal with manually here\n",
    "iran_data.loc[iran_data['county_en'] == 'Yasooj\\r', 'county_en'] = 'Yasooj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in human data and rename columns\n",
    "human_data = pd.read_csv(os.path.join(fp, 'Data', 'Human_Brucellosis_2015-2018_V3.csv'))\n",
    "human_data = human_data.rename(columns = {'Urban/Rural/Itinerant/Nomadic':'Pop_setting',\n",
    "                                          'Prepnancy':'Pregnancy',\n",
    "                                          'Occuptio':'Occupation',\n",
    "                                          'Livestock interaction history':'Livestock_int_hist',\n",
    "                                          'Livestock interaction type':'Livestock_int_type',\n",
    "                                          'Unpasteurized dairy consumption ':'Unpast_dairy',\n",
    "                                          'Other family members infection':'Fam_members_inf',\n",
    "                                          'Outbreak Year':'Outbreak_yr',\n",
    "                                          'Outbreak Month':'Outbreak_mth',\n",
    "                                          'Diagnosis Year':'Diagnosis_yr',\n",
    "                                          'Diagnosis Month':'Diagnosis_mth',\n",
    "                                          'Livestock vaccination history':'Livestock_vac_hist'})\n",
    "\n",
    "## Fix duplicate provinces (present both capitalized and uncapitalized)\n",
    "human_data.loc[human_data['Province'] == 'Khorasan jonobi', 'Province'] = 'Khorasan Jonobi'\n",
    "human_data.loc[human_data['Province'] == 'Khorasan shomali', 'Province'] = 'Khorasan Shomali'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socioeconomic status data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in SES data and rename columns\n",
    "ses_data = pd.read_csv(os.path.join(fp, 'Data','ses_data.csv'))[['province', 'pop', 'hshld_size', 'ses']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County-level population data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in population data\n",
    "pop_data = pd.read_csv(os.path.join(fp, 'Data', 'pop_by_county.csv'), skiprows = [1, 2, 3, 4], usecols = [0, 1])\n",
    "\n",
    "## Remove unneeded rows\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Setteled\", case = False)].index)\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Settled\", case = False)].index)\n",
    "\n",
    "## Trim strings\n",
    "pop_data['Description'] = pop_data['Description'].str.strip()\n",
    "pop_data['Population'] = pop_data['Population'].str.replace(',', '').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Name Discrepancies\n",
    "\n",
    "Of course, working with data from a variety of sources requires extensive data preparation and cleaning to ensure the correct observations are matched when merging the data into a single source.\n",
    "\n",
    "We discovered quickly that translations of province and county names from Persian to English varied widely across the datasets. This presented serious problems for our ability to merge datasets, as even single character differences between county names across the datasets would prevent the data from being matched properly. However, given the more than 400 distinct counties in Iran, it was impractical to attempt to manually identify the discrepancies and update the names accordingly. To speed up our ability to identify county name discrepancies, we defined three functions to help identify many of the matches automatically:\n",
    "\n",
    "- match_names\n",
    "- likely_matches\n",
    "- map_caps\n",
    "\n",
    "### Function: match_names\n",
    "\n",
    "match_names is a function that iterates through two user-supplied lists of strings and calculates the Levenshtein distance and Levenshtein ratio between the distinct pairings of strings between the two lists. It then assembles a dataframe displaying the matches that each method (Levenshtein distance or Levenshtein ratio) identifies as the best possible match. The function will find a potential match for all strings in the first list, but will not necessarily find a match for all strings in the second list passed.\n",
    "\n",
    "The Levenshtein distance and Levenshtein ratio are string similarity metrics that rely on the number of individual character changes are required to convert one string into another. More information can be found [here](https://en.wikipedia.org/wiki/Levenshtein_distance).\n",
    "\n",
    "There are a few options for the user to specify:\n",
    "\n",
    "- __as_df:__ if True, returns a single dataframe with the automatically identified optimum matches. If False, returns a dictionary of where each string in the first list is represented as a key. The associated values are dataframes of all the Levenshtein distances and ratios for that key string when matched to each word in the second list. (This allows for more granular investigation when the top match seems questionable.)\n",
    "- __caps:__ if True, standardizes capitalization of all strings before matching. May improve match success depending on the format of the strings passed to the function\n",
    "- __unique:__ if True, the function only attempts to match names that don't already have a perfect match. This usually improves accuracy since it removes the possiblity of matching a string in the first list to a value that already has a perfect match to another string in the second list. However, when a one-to-one matching isn't expected, setting this to False may improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to match potential misspelled strings\n",
    "## Takes two lists, calculates Levenshtein distance and ratio to identify potential matches\n",
    "## Used in the likely_matches function\n",
    "def match_names(s1, s2, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    s2 = pd.Series(s2)\n",
    "    \n",
    "    ## Get unique values in each series\n",
    "    vals1 = s1.unique()\n",
    "    vals2 = s2.unique()\n",
    "    \n",
    "    ## If unique argument is set to true, only match names that don't already have perfect match\n",
    "    if unique == True:\n",
    "        \n",
    "        ## Unique values in series 1 that aren't in series 2\n",
    "        vals1 = pd.Series(np.setdiff1d(vals1, vals2))\n",
    "    \n",
    "        ## Unique values in series 2 that aren't in series 1\n",
    "        vals2 = pd.Series(np.setdiff1d(vals2, vals1))\n",
    "        \n",
    "    if caps == True:\n",
    "        \n",
    "        ## Capitalize before matching\n",
    "        vals1 = vals1.str.capitalize()\n",
    "        vals2 = vals2.str.capitalize()\n",
    "        \n",
    "    ## Calculate Levenshtein distance and ratio\n",
    "    dists = np.array([leven.distance(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "    ratios = np.array([leven.ratio(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "        \n",
    "    ## Reshape and convert to df so we can identify which values are for which name combo\n",
    "    dists_df = pd.DataFrame(data = dists.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    ratios_df = pd.DataFrame(data = ratios.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    \n",
    "    if as_df == True:\n",
    "        \n",
    "        ## Get column names where min distance and max ratio occurs\n",
    "        matches = pd.DataFrame({'name_dist': dists_df.idxmin(axis = 1), \n",
    "                                'name_ratio': ratios_df.idxmax(axis = 1), \n",
    "                                'dist': dists_df.min(axis = 1), \n",
    "                                'ratio': ratios_df.max(axis = 1)})\n",
    "    \n",
    "        return(matches)\n",
    "        \n",
    "    ## If user wants more detail, we can create a dictionary for each name that contains more graunlar info\n",
    "    ## This section creates a dictionary with names as keys and dataframes as values\n",
    "    ## Each dataframe contains all the possible name pairings (as opposed to above, which only supplies best possible values)\n",
    "    else:\n",
    "        \n",
    "        ratios_dict = {name1: ratios_df.loc[name1].sort_values(ascending = False) for name1 in vals1}\n",
    "        dists_dict = {name1: dists_df.loc[name1].sort_values() for name1 in vals1}\n",
    "        \n",
    "        comb_dict = {name1: pd.merge(dists_dict[name1], ratios_dict[name1], left_index = True, right_index = True, suffixes=('_dist', '_ratio')) for name1 in vals1}\n",
    "        \n",
    "        return(comb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: likely_matches\n",
    "\n",
    "likely_matches uses the match_names function to identify potential name matches, but also checks to determine whether the optimum match identified by the Levenshtein distance method and Levenshtein ratio method are the same. If both of these guesses are the same and the ratio is sufficiently high, the function populates a dataframe identifying those matches in a new column. This allows the user to easily identify which strings had accurate matches and which remain unmatched (either because the Levenshtein ratio is not sufficiently high or the two methods disagree about the best match).\n",
    "\n",
    "In addition to the arguments that can be specified in match_names, likely_matches also allows the user to specify the ratio cutoff that will be used to classify strings as matched or unmatched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to return highly probable string matches - the rest will have to be done manually\n",
    "## Depends on match_names function\n",
    "## The index of the resulting df contains values from the *first* series passed to the function\n",
    "def likely_matches(s1, s2, cutoff = 0.75, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    ## Create dataframe recording potential name matches\n",
    "    matched = match_names(s1, s2, as_df = as_df, caps = caps, unique = unique)\n",
    "    \n",
    "    ## Add column recording whether distance and ratio identify the same match\n",
    "    matched['name_match'] = (matched['name_dist'] == matched['name_ratio'])\n",
    "    \n",
    "    ## Can be highly confident when nameMatch = True, ratio >= .75 - this matches 145 of the 192 that need matches\n",
    "    matched['matched'] = np.where((matched['name_match'] == True) & (matched['ratio'] >= cutoff), matched['name_dist'], 'NULL')\n",
    "           \n",
    "    return(matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: map_caps\n",
    "\n",
    "This is a small helper function to be used in conjunction with match_names and likely_matches. When caps is set to True in either of those functions, strings are capitalized before matching. We found that this typically improves matching accuracy, but it is important to be able to revert the strings back to their original form before the capitalization. \n",
    "\n",
    "map_caps simply creates a dictionary mapping a list of strings to their capitalized values. Then, regardless of whether match_names and likely_matches are used with capitalization or not, the user can easily find the original string values before the matching process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Little helper function that creates a dictionary mapping capitalized values\n",
    "## to original values (so we can go back and forth more easily)\n",
    "def map_caps(s1):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    \n",
    "    ## Capitalize series values\n",
    "    s1_caps = s1.str.capitalize().unique()\n",
    "    \n",
    "    ## Map original values to capitalized values\n",
    "    caps_mappings = {name1:name2 for name1, name2 in zip(s1_caps, s1.unique())}\n",
    "    \n",
    "    return(caps_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Province and County Name Matching\n",
    "\n",
    "We split the process of matching names for each dataset into province names and county names. For the human data, the likely_matches function only auto-identified 4 matches, but given that all but 10 provinces were consistent across the human data and iran shapefile at the beginning, it was simple to manually match the remaining 6 names. We created a dictionary to store the mappings from the human data province names to their shapefile counterparts and updated the human data dataframe with the corresponding matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Province Matching for human data\n",
    "provs1 = human_data.loc[human_data['Province'] != 'Null']['Province']\n",
    "provs2 = iran_data['province_en']\n",
    "\n",
    "likely_matches(provs1, provs2, caps = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'Azarbaijan Gharbi',\n",
    "    'East Azerbaijan':'Azarbaijan Sharghi',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh & Boyerahmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan'\n",
    "              }\n",
    "\n",
    "## Invert dictionary - want keys to be from the human data\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "human_data['Province'] = human_data['Province'].map(match_dict_prov).fillna(human_data['Province'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of matching counties was more laborious since there were far more counties that needed to be matched, which made it difficult to confirm matches through visual inspection. Still, the likely_matches function reduced the discrepant name pairs from more than 200 to 40 that required manual matching. After populating a dictionary with the automatched names and the manually matched names, we updated the county names in the human data to be consistent with the shapefile county names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County Matching\n",
    "## Remove null values for matching\n",
    "counties1 = human_data.loc[human_data['County'] != 'Null']['County']\n",
    "counties2 = iran_data['county_en']\n",
    "\n",
    "## Create mapping of likely pairs\n",
    "matched_df = likely_matches(counties1, counties2)\n",
    "#match_names(counties1, counties2, as_df = False)\n",
    "\n",
    "#matched_df[matched_df['matched'] == 'NULL']\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "\n",
    "## Revert automatched names back to original form data files and zip matched pairs into dictionary\n",
    "caps_mappings1 = map_caps(counties1)\n",
    "caps_mappings2 = map_caps(counties2)\n",
    "\n",
    "match_dict_cty = dict(zip(automatched.index.map(caps_mappings1), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manual matching\n",
    "match_dict_man = {\n",
    "    'Ali Abad Katul':'Aliabad', \n",
    "    'Bafgh':'Bafq', \n",
    "    'Bandar Qaz':'Bandar-e-Gaz', \n",
    "    'Dailam':'Deylam',\n",
    "    'Gonbad  kavoos':'Gonbad-e-Kavus', \n",
    "    'Ijroud':'Eejrud', \n",
    "    'Jovein':'Jowayin',\n",
    "    'Kalale':'Kolaleh',\n",
    "    'Mahvalat':'Mahvelat', \n",
    "    'Menojan':'Manujan', ## auto-matched incorrectly\n",
    "    'Neyshabur':'Nishapur',  \n",
    "    'Orzoieyeh':'Arzuiyeh', \n",
    "    'Ray':'Rey', \n",
    "    'Tehran Jonub':'Tehran', \n",
    "    'Tehran Shomal':'Tehran', \n",
    "    'Tiran o Karvan':'Tiran-o-Korun',\n",
    "    'Abadeh Tashk':'Abadeh',\n",
    "    'Agh Ghala':'Aqqala',\n",
    "    'Ahvaz e gharb':'Ahvaz',\n",
    "    'Ahvaz e Shargh':'Ahvaz',\n",
    "    'Gilan Qarb':'Gilan-e-Gharb',\n",
    "    'Kharame':'Kherameh',\n",
    "    'Maraqe':'Maragheh',\n",
    "    'Tehran Gharb':'Tehran',\n",
    "    'Tehran Shargh':'Tehran',\n",
    "    'Tehran Shomal Qarb':'Tehran',\n",
    "    'Zaveh':'Zave',\n",
    "    'Bandar Mahshahr':'Mahshahr',\n",
    "    'Qale ganj':'Ghaleye-Ganj',\n",
    "    'Sarchahan':'Hajiabad',\n",
    "    'Kamfirouz':'Marvdasht',\n",
    "    'Zarghan': 'Shiraz',\n",
    "    'Beyza':'Sepidan',\n",
    "    'Dore Chagni':'Doureh',\n",
    "    'Sepid Dasht':'Khorramabad',\n",
    "    'Nour Abad':'Mamasani',\n",
    "    'Aleshtar':'Selseleh',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Saduq':'Yazd',\n",
    "    'Mashhad Morghab':'Khorrambid',\n",
    "    'Dehdez':'Izeh',\n",
    "    'zaboli':'Mehrestan',\n",
    "    'Qaemiyeh':'Kazerun',\n",
    "    'Samen ol Aemmeh':'Mashhad',\n",
    "    'kish':'Bandar-Lengeh'\n",
    "              }\n",
    "\n",
    "## Add manually matched names to the mapping dictionary\n",
    "match_dict_cty.update(match_dict_man) \n",
    "\n",
    "## Add all the perfect matches to this dictionary\n",
    "## Mapping dictionary now should include all matches\n",
    "perf_matches = np.intersect1d(iran_data['county_en'].str.capitalize(), human_data['County'].str.capitalize())\n",
    "match_dict_cty.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Map names in dataframe based on dictionary\n",
    "human_data['County'] = human_data['County'].map(match_dict_cty).fillna(human_data['County'])\n",
    "\n",
    "## Joining ##\n",
    "human_sp_data = pd.merge(human_data, iran_data, how = 'outer', left_on = 'County', right_on = 'county_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the animal data (and other datasets) could potentially have names inconsistent with both the human data and the shapefile, we needed to perform a similar procedure for each dataset to be merged. Following is the code to pair province names and county names for the animal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Animal data province matching\n",
    "\n",
    "#provs1 = animal_data['province']\n",
    "#provs2 = iran_data['province_en']\n",
    "\n",
    "#likely_matches(provs1, provs2, caps = False)\n",
    "#match_names(provs1, provs2, as_df = True, caps = False, unique = True)\n",
    "\n",
    "## Province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'West Azarbayjan',\n",
    "    'East Azerbaijan':'East Azarbayjan',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan',\n",
    "    'Hamadan':'Hamedan',\n",
    "    'Kermanshan':'Kermanshah',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh and BoyerAhmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Kerman':'South Kerman'\n",
    "              }\n",
    "\n",
    "## Invert dictionary\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "animal_data['province'] = animal_data['province'].map(match_dict_prov).fillna(animal_data['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County names ##\n",
    "ani_cnties = animal_data['county']\n",
    "\n",
    "matched_df = likely_matches(ani_cnties, iran_data['county_en'])\n",
    "#match_names(ani_cnties, counties2, as_df = False)\n",
    "\n",
    "ani_caps_mappings = map_caps(ani_cnties)\n",
    "\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "#unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start mapping dictionary with the automatched names\n",
    "match_dict_ani = dict(zip(automatched.index.map(ani_caps_mappings), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_ani = {\n",
    "    'Aran and Bidgol':'Aran-o-Bidgol',\n",
    "    'Buin and Miandasht':'Booeino Miyandasht',\n",
    "    'Deyr':'Dayyer',\n",
    "    'Haftkel':'Haftgol',\n",
    "    'Ijrud':'Eejrud',\n",
    "    'Maneh asd Samalgan':'Maneh-o-Samalqan',\n",
    "    'Orzueeyeh':'Arzuiyeh',\n",
    "    'Qaleh Ganj':'Ghaleye-Ganj',\n",
    "    'Qir and Karzin':'Qir-o-Karzin',\n",
    "    'Raz and Jargalan':'Razo Jalgelan',\n",
    "    'Sib and Suran':'Sibo Soran',\n",
    "    'Tiran and Karvan':'Tiran-o-Korun',\n",
    "    'Torqebeh and Shandiz(Binalud)':'Torghabe-o-Shandiz',\n",
    "    'Zaveh':'Zave',\n",
    "    'Chardavol':'Shirvan-o-Chardavol',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'mahshahr':'Mahshahr',\n",
    "    'Jafarieh':'Torbat-e-Jam',\n",
    "    'Kahak':'Sabzevar',\n",
    "    'Kohgiluyeh and BoyerAhmad':'Kohgeluyeh',\n",
    "    'BoyerAhmad':'Yasooj'\n",
    "              }\n",
    "\n",
    "match_dict_ani.update(match_dict_man_ani)\n",
    "\n",
    "#unmatched2 = [name for name in unmatched.index.map(ani_caps_mappings) if not name in match_dict_ani.keys()]\n",
    "\n",
    "## Add identical matches to the dictionary\n",
    "perf_matches = np.intersect1d(iran_data['county_en'], animal_data['county'])\n",
    "match_dict_ani.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Update county names in animal data and do the join\n",
    "animal_data['county'] = animal_data['county'].map(match_dict_ani).fillna(animal_data['county'])\n",
    "ani_sp_data = pd.merge(animal_data, iran_data, how = 'outer', left_on = 'county', right_on = 'county_en')\n",
    "\n",
    "## Sum infection data grouped by county, year, and month and remerge on animal data\n",
    "ani_sp_data_grp = ani_sp_data.groupby(['county', 'year', 'month'], as_index = False)[ani_sp_data.columns[10:15]].sum()\n",
    "ani_sp_data = pd.merge(ani_sp_data, ani_sp_data_grp, how = 'left')\n",
    "\n",
    "## Calculate infection rate\n",
    "ani_sp_data['animal_inf_rate'] = ani_sp_data['n_infected']/ani_sp_data['n_sample']\n",
    "\n",
    "## Write mapping dictionary to csv for ease of QA\n",
    "# pd.DataFrame.from_dict(data=match_dict_ani, orient='index').to_csv(fp + '/animal_data_mappings.csv', index_label = ['animal_county'], header = ['shp_county'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The socioeconomic status data fortunately was accounted for by the matches identified automatically by the likely_matches function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get province names\n",
    "ses_provs = ses_data['province'].unique()\n",
    "\n",
    "## Automatch ses names to spatial data province names\n",
    "ses_matches = likely_matches(ses_provs, iran_data['province_en'].unique())\n",
    "\n",
    "## Maps from matched names back to uncapitalized names\n",
    "ses_caps_map = map_caps(ses_provs)\n",
    "iran_prov_map = map_caps(iran_data['province_en'].unique())\n",
    "\n",
    "## Determine names that were successfully matched\n",
    "automatched = ses_matches[ses_matches['matched'] != 'NULL']\n",
    "\n",
    "## Create dictionary mapping ses names to spatial data names and update ses_data\n",
    "match_dict_ses = dict(zip(automatched.index.map(ses_caps_map), automatched['matched'].map(iran_prov_map)))\n",
    "ses_data['province'] = ses_data['province'].map(match_dict_ses).fillna(ses_data['province'])\n",
    "\n",
    "## Join data\n",
    "ses_sp_data = pd.merge(ses_data, iran_data, how = 'outer', left_on = 'province', right_on = 'province_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population data proceeded in a similar manner, but province names and county names were not separated in the population csv file. So, we needed to match all names simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to append provinces and counties from iran_data because the pop data is not separated by prov/county\n",
    "provs = iran_data['province_en']\n",
    "cts = iran_data['county_en']\n",
    "\n",
    "all_names = provs.append(cts)\n",
    "\n",
    "## Auto-match names\n",
    "matched_df = likely_matches(pop_data['Description'], all_names)\n",
    "\n",
    "## Map original data names to capitalized names\n",
    "pop_caps_mappings = map_caps(pop_data['Description'])\n",
    "iran_caps_mappings = map_caps(all_names)\n",
    "\n",
    "## Identify automatched and unmatched names\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start match dictionary with automatched names\n",
    "match_dict_pop = dict(zip(automatched.index.map(pop_caps_mappings), automatched['matched'].map(iran_caps_mappings)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_pop = {\n",
    "    'Arzooeyeh':'Arzuiyeh',\n",
    "    'Bafgh':'Bafq',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Firooze':'Firuzeh',\n",
    "    'Ijerud':'Eejrud',\n",
    "    'Ivan':'Eyvan',\n",
    "    'Jovin':'Jowayin',\n",
    "    'Mayamee':'Meyami',\n",
    "    'Naeen':'Nain',\n",
    "    'Neemrooz':'Nimrouz',\n",
    "    'Neyshabur':'Nishapur',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'Zaveh':'Zave',\n",
    "    'Khorasan-e-Razavi':'Razavi Khorasan',\n",
    "    'Qaleh-Ganj':'Ghaleye-Ganj',\n",
    "    'Qaser-e Qand':'Ghasre Ghand',\n",
    "    'Raz & Jargalan':'Razo Jalgelan',\n",
    "    'Reegan':'Rigan',\n",
    "    'Savadkuh-e Shomali':'Northern Savadkooh',\n",
    "    'Sireek':'Sirik',\n",
    "    'Sumaehsara':'Some\\'e-Sara',\n",
    "    'Tiran & Karvan':'Tiran-o-Korun',\n",
    "    'Zeerkooh':'Zirkouh',\n",
    "    'Bandar-e-Mahshahr':'Mahshahr',\n",
    "    'Bon':'Ben',\n",
    "    'Chardavel':'Shirvan-o-Chardavol',\n",
    "    'Fonuch':'Fanouj',\n",
    "    'Keyar':'Kiaar',\n",
    "    'Qayenat':'Qaen',\n",
    "    'Qods':'Shahr-e Qods',\n",
    "    'Sibsavaran':'Sibo Soran',\n",
    "    'Kordestan':'Kurdistan',\n",
    "    'Binalood':'Torghabe-o-Shandiz',\n",
    "    'Nayer':'Nir',\n",
    "    'Orumiyeh':'Urumia',\n",
    "    'Khosaf':'Khusf'\n",
    "              }\n",
    "\n",
    "## Update dictionary with manual matches\n",
    "match_dict_pop.update(match_dict_man_pop)\n",
    "\n",
    "## Still unmatched:\n",
    "#unmatched2 = [name for name in unmatched.index.map(pop_caps_mappings) if not name in match_dict_pop.keys()]\n",
    "\n",
    "## Add identical matches to dictionary\n",
    "perf_matches = np.intersect1d(all_names, pop_data['Description'])\n",
    "match_dict_pop.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "pop_data['Mapped'] = pop_data['Description'].map(match_dict_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are cases where a county has the same name as its province, so we also had to identify which entries were associated with provinces and which were associated with counties even after successfully matching names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disentangling instances where provinces and county names match (since they're all lumped together in this case):\n",
    "## Identify duplicate names\n",
    "dup_names = pop_data[pd.DataFrame.duplicated(pop_data, 'Description')][['Description','Mapped']]\n",
    "\n",
    "## Sort by population. We know that the first entry for each name will be the province pop\n",
    "## and the second entry will be the county prop since prov pop >= county pop\n",
    "dup_vals = pop_data[pop_data['Description'].isin(dup_names['Description'])].sort_values(by = ['Description','Population'], ascending=[True, False])\n",
    "dup_vals['Geog_region'] = np.resize(['Province','County'], len(dup_vals))\n",
    "\n",
    "## Tagging each name as either province or county\n",
    "dup_pop_data_provs = dup_vals[dup_vals['Geog_region'] == 'Province']\n",
    "nondup_pop_data_provs = pop_data[pop_data['Mapped'].isin(provs.sort_values().unique()) & (~pop_data['Description'].isin(dup_pop_data_provs['Description']))]\n",
    "nondup_pop_data_provs['Geog_region'] = 'Province'\n",
    "\n",
    "## Merge back on population data - now everything is tagged to indicate province or county\n",
    "merge1 = pd.merge(nondup_pop_data_provs, dup_vals, how='outer')\n",
    "merge2 = pd.merge(merge1, pop_data, how='outer')\n",
    "\n",
    "## Drop provinces - we only want counties\n",
    "pop_data_cts_only = merge2[merge2['Geog_region']!='Province'][['Mapped','Population']]\n",
    "\n",
    "## Merge with spatial data on county name\n",
    "pop_sp_data = pd.merge(pop_data_cts_only, iran_data, how = 'outer', left_on = 'Mapped', right_on = 'county_en')\n",
    "\n",
    "## Drop erroneous row - results from original pop_data file having this entry listed twice.\n",
    "pop_sp_data = pop_sp_data[pop_sp_data['Mapped']!='Razavi Khorasan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data aggregation \n",
    "\n",
    "In this stage, the previously refined data will be aggregated to reach a single number representing a total number of disease cases for each county. The output of this step would be a geo data frame holding a name, population, total disease cases, and a geometry per county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the preprocessed data\n",
    "df = human_sp_data\n",
    "df_pop = pop_sp_data\n",
    "cum = 0\n",
    "coun = []\n",
    "agg = []\n",
    "geom = []\n",
    "pop = []\n",
    "j = 0\n",
    "\n",
    "for cty in df['county_en'].unique(): # Looping over county names\n",
    "    for i in range(len(df)): # Looping over data rows\n",
    "        if df.iloc[i, -7] == cty: # checking to see if the county name for a patient record matches the county name in the current loop\n",
    "            if cum == 0: # Taking the county name only once \n",
    "                coun.append(df.iloc[i, -7]) \n",
    "                for p in range(len(df_pop)): #Looping over the other population data \n",
    "                    if df_pop.iloc[p, 2] == cty: # checking to see if the county names match\n",
    "                        pop.append(df_pop.iloc[p, 1]) # getting the population value for each county \n",
    "                        geom.append(df_pop.iloc[p, -1]) # taking the geometry from pop data frame\n",
    "                    \n",
    "            cum += 1 # summing up the number of patients per county\n",
    "    \n",
    "    agg.append(cum) # Appending the disease population for each county\n",
    "    cum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a pandas data frame for derived columns\n",
    "ag_data = pd.DataFrame([coun, pop, agg, geom]).T\n",
    "headers = ['county_en', 'population', 'bruc', 'geometry']\n",
    "ag_data.columns = headers\n",
    "\n",
    "# Deteling the rows having NaN geometries due to table inconsistencies/data duplications\n",
    "ag_data2 = ag_data.drop([429,430,431])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation and Hotspot Detection \n",
    "\n",
    "### Global Moran's I\n",
    "\n",
    "To investigate the type of spatial pattern (i.e., random, uniform, and clustered) of the disease incidence in Iran, global Moran’s I analysis is employed. The Global Moran’s I measures the spatial autocorrelation of the cities. \n",
    "The expected output of this step will be an index ranging between -1 and 1, where -1 represents a dispersed distribution, 0, a random and 1 shows a clusterred distribution status. \n",
    "\n",
    "### Spatial Weights Matrix (w)\n",
    "\n",
    "For this research, the Queen neighbourhood strategy is considered. \n",
    "\n",
    "### Note:\n",
    "\n",
    "This section of code began behaving erratically after we combined our various sections of code into a single notebook. At this point, we unexpectedly ran into a new error (\"no supported conversion for types: (dtype('float64'), dtype('O')\") that we will seek to resolve after the prototype stage. Hopefully, this won't be an issue, but if so, we've captured the resulting images that we got from the code and have included them in the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional modules needed\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights.contiguity import Queen\n",
    "import splot\n",
    "from esda.moran import Moran\n",
    "from splot.esda import moran_scatterplot # loading Moran's scatterplot module\n",
    "from splot.esda import plot_moran\n",
    "from splot.esda import moran_scatterplot\n",
    "from esda.moran import Moran_Local\n",
    "from splot.esda import lisa_cluster\n",
    "from splot.esda import plot_local_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Global_morans_I(gdf):\n",
    "    y = gdf['bruc'].values # Choosing the value of interest. Here: the total number of disease cases for each county. \n",
    "    # Creating a Queen based contiguity weights matrix\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "    w.transform = 'r'\n",
    "    \n",
    "\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "    moran = Moran(y, w) #Calling Moran's I function and passing the value y and the neighbourhood matrix w\n",
    "    print('The global Moran\\'s index is', moran.I)\n",
    "    \n",
    "    fig, ax = moran_scatterplot(moran, aspect_equal=True) # Calling the function and passing the calculated Moran's I\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    #Plotting Moran's I diagram for our case to see how significant is our positive autocorrelation. \n",
    "    plot_moran(moran, zstandard=True, figsize=(10,4))\n",
    "    plt.show()\n",
    "    \n",
    "    #Plotting the p-value of the result. p-value is a statistical measure of significance. \n",
    "    # any p-value smaller than 0.05 would be considerred statistically significant for the level of at least %95. \n",
    "    print('The p-value is',moran.p_sim)\n",
    "\n",
    "Global_morans_I(ag_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Local_morans_I(gdf):\n",
    "    y = gdf['bruc'].values # Choosing the value of interest. Here: the total number of disease cases for each county. \n",
    "    # Creating a Queen based contiguity weights matrix\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "\n",
    "# calculate Local Moran's I to detect the disease hot and cold spots\n",
    "    moran_loc = Moran_Local(y, w) \n",
    "    fig, ax = moran_scatterplot(moran_loc)\n",
    "    ax.set_xlabel('Disease Total Incidence')\n",
    "    ax.set_ylabel('Spatial Lag of Incidences')\n",
    "    plt.show()\n",
    "    \n",
    "    # Running Moran's scatterplot function for the computed local Moran's I. \n",
    "    fig, ax = moran_scatterplot(moran_loc, p=0.05)\n",
    "    ax.set_xlabel('Disease Total Incidence')\n",
    "    ax.set_ylabel('Spatial Lag of Incidences')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# plotting calculated local Moran's I map. \n",
    "    lisa_cluster(moran_loc, gdf, p=0.05, figsize = (9,9))\n",
    "    plt.show()\n",
    "    \n",
    "    # plotting local Moran's I result and the scatter plot for a better interpretation.\n",
    "    \n",
    "    plot_local_autocorrelation(moran_loc, gdf, 'total')\n",
    "    plt.show()\n",
    "    \n",
    "Local_morans_I(ag_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results of local Moran's I analysis, there are two major disease hotspot with a statistical significance of 95%. One of the disease clusters is located in northeastern parts of the country and the second one is situate in the northwest. \n",
    "This analysis also revealed Low-Low clusters which are the significant coldspots of the disease throughout the country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding environmental Data\n",
    "The environmental data is stored in a separate csv after being pulled from Google Earth Engine. The data is read in as a pandas  dataframe indexed by county with columns for each environmental variable for each month from 1996-2018. The data is aggregated as mean value by month.\n",
    "\n",
    "The data was aggregated using the iran shapefile, so the county names already match those in the human and animal dataframes and do not need to be changed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: addGregorian\n",
    "The animal data contains only the jalali calendar date in the form of a jalali month and jalali year column. This function takes the dataframe, the year column name and the month column name and edits the dataframe in-place to add a gregorian year and month column. It utilizes the jdatetime module which easily converts jalali dates to gregorian.\n",
    "\n",
    "#### pandas.apply\n",
    ".apply() iterates a function over each row in a dataframe if axis=1 is specified and returns a series. In this case, it is used to create a new series which will be the year or month column.\n",
    "\n",
    "#### lambda functions\n",
    "Lambda functions are anonymous, meaning they are not given a name. In this case, they are useful because we need only a simple function for a short period of time. In the case of the first lambda below, it could also be written as:\n",
    "\n",
    "def myfun(row):\n",
    "\n",
    "    if (row[yearCol].isnumeric() and row[moCol].isnumeric()):\n",
    "        jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian().strftime('%y')\n",
    "    else:\n",
    "        None\n",
    "\n",
    ".apply() applies this function to each row to generate the gregorian year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdatetime\n",
    "\n",
    "def addGregorian(data, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Adds gregorian month and year columns to a dataframe based on jalali month and year columns. \n",
    "    Assumes first day of jalali month, since day is not given.\n",
    "    \"\"\"\n",
    "    #The apply function iterates a function over each row in a dataframe (axis=1 specifies row not column)\n",
    "    #The function creates a jalali date object for each row from the jalali year and month. \n",
    "    #Then, it converts it to a gregorian date and extracts the year or month. \n",
    "    #If the jalali year and date are not numeric (e.g. Null as a string) then the year and month get None values. \n",
    "    data['year']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian()\\\n",
    "                            .strftime('%y') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)\n",
    "    data['month']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian()\\\n",
    "                             .strftime('%m') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: addEnvData\n",
    "The environmental dataframe is indexed by county and has column names following the \"variableName_YYYYMM\" format. The following are some column names in the dataframe: \n",
    "\n",
    "mean_2m_air_temperature_200801\n",
    "\n",
    "mean_total_precipitation_201008\n",
    "\n",
    "mean_ndvi_201811\n",
    "\n",
    "Because elevation doesn't typically change with time, there is only one elevation column named mean_elevation.\n",
    "\n",
    "#### .apply() and lambda functions\n",
    "Just like addGregorian(), addEnvData() uses apply and lambda functions to extract data from the environmental dataframe.\n",
    "#### row.County\n",
    "row.County is used to get the county name of the row, and to locate the corresponding row in the environmental dataframe.\n",
    "#### Appending YYYYMM to end of column name\n",
    "For each row, the _YYYYMM sequence is added to the end of the variable name to choose data from the correct column in the environmental dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEnvData(data, envDF, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Takes two dataframes: \n",
    "    data-  a dataframe indexed by county, contains at least a year and month column\n",
    "    envDF- a dataframe indexed by county, contains  columns for each environmental variable \n",
    "           for each month from 1996-2018. The data is aggregated as mean by month. \n",
    "    \n",
    "    The function pulls the corresponding county/date value for each environmental data variable\n",
    "    and adds them as a new column in 'data'.\n",
    "    \"\"\"\n",
    "    #The apply function iterates a function over each row in a dataframe (axis=1 specifies row)\n",
    "    #envDF.loc[row.County] sorts the environmental data to the correct county. \n",
    "    #The next bracket selects a column by creating a datestring in the correct format. .zfill is required to make sure months are in '04' format instead of '4'\n",
    "    data['mean_2m_air_temperature'] = data.apply(lambda row: envDF.loc[row.County]['mean_2m_air_temperature_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_total_precipitation'] = data.apply(lambda row: envDF.loc[row.County]['total_precipitation_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_ndvi'] = data.apply(lambda row: envDF.loc[row.County]['mean_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    #Elevation doesn't change with time, so the date selector is not necessary\n",
    "    data['mean_elevation'] = data.apply(lambda row: envDF.loc[row.County]['mean_elevation'] if (row.County in envDF.index) else None, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human data needs to be cleaned just a little bit. Some float-type NaN values need to be converted to 'Null' strings, otherwise the functions above won't function.\n",
    "\n",
    "For the animal data, the 'county' column just needs to be capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Environmental Data\n",
    "envFP=os.path.join(fp, 'Data', 'allParams.csv')\n",
    "envData = pd.read_csv(envFP, index_col='ADM2_EN')\n",
    "\n",
    "## Clean up human data, add date column\n",
    "human_sp_data.loc[pd.isna(human_sp_data['Outbreak_yr']), 'Outbreak_yr']='Null'\n",
    "addGregorian(human_sp_data, 'Outbreak_yr', 'Outbreak_mth')\n",
    "\n",
    "## Clean up animal data\n",
    "ani_sp_data.rename(columns={\"county\": \"County\"}, inplace=True)\n",
    "\n",
    "## Adding environmental data to both dataframes\n",
    "addEnvData(ani_sp_data, envData, 'year', 'month')\n",
    "addEnvData(human_sp_data, envData, 'year', 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Future Steps\n",
    "\n",
    "- The steps ahead of this project would be: \n",
    "    - Incorporating animal vaccination \n",
    "    - Assessing possible correlation between animal and human data\n",
    "    - Implementing a multiple linear regression analysis incorporating environmental variables and the standard disease incidence per 100000 people per county. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some extra maps that we can put in wherever it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregating data for choropleth plotting\n",
    "counts_by_group = human_sp_data.groupby(['county_en','province_en', 'Livestock_int_hist','Livestock_vac_hist','Pop_setting']).size().reset_index(name='count') \n",
    "county_obs = human_sp_data.groupby(['county_en']).size().reset_index(name='county_count')\n",
    "\n",
    "count_df = pd.merge(counts_by_group, county_obs)\n",
    "\n",
    "count_df = pd.merge(count_df, pop_sp_data[['county_en','Population']], how='outer', on ='county_en')\n",
    "\n",
    "count_df['inf_rate'] = count_df['count']/count_df['Population']\n",
    "\n",
    "subset_data = pd.merge(count_df, ses_sp_data[['county_en','ses']], how='outer', on='county_en')\n",
    "\n",
    "#subset_data.to_csv(os.path.join(fp, 'Data', 'dataForKatharine2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "plot_data = subset_data.copy()\n",
    "plot_data = plot_data.loc[~plot_data['province_en'].isnull()]\n",
    "iran_json = iran_data.to_json()\n",
    "\n",
    "## log transform to get better sense of variation\n",
    "plot_data['inf_rate_log'] = np.log(plot_data['inf_rate'])\n",
    "\n",
    "cty_map = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data = iran_json,\n",
    "    name = 'choropleth',\n",
    "    data = plot_data,\n",
    "    columns = ['county_en', 'inf_rate_log'],\n",
    "    key_on = 'feature.properties.county_en',\n",
    "    fill_color = 'YlOrRd',\n",
    "    line_color = 'black',\n",
    "    fill_opacity = 0.7,\n",
    "    line_opacity = 0.2,\n",
    "    legend_name = 'Infection Rate'\n",
    ").add_to(cty_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create map of iran\n",
    "prov_map = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "## Map infection rate at county level\n",
    "folium.Choropleth(\n",
    "    geo_data = iran_json,\n",
    "    name = 'choropleth',\n",
    "    data = plot_data,\n",
    "    columns = ['province_en', 'inf_rate_log'],\n",
    "    key_on = 'feature.properties.province_en',\n",
    "    fill_color = 'YlOrRd',\n",
    "    line_color = 'black',\n",
    "    fill_opacity = 0.7,\n",
    "    line_opacity = 0.2,\n",
    "    legend_name = 'Infection Rate (%)'\n",
    ").add_to(prov_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high levels of clustering suggested by the plots lead us to investigate a barplot of the infection rate by county to more easily determine whether there are outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## More flexibility in folium when mapping straight from geojson \n",
    "## so load up the iran shapefile data in new format\n",
    "county_geo = f'{fp}/iran_data.json'\n",
    "\n",
    "with open(county_geo) as json_file:\n",
    "    iran_geodata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiailize continuous color scale for log infection rate\n",
    "import branca\n",
    "\n",
    "plot_data[plot_data['Livestock_vac_hist']=='No']\n",
    "plot_data['county_inf_rate'] = plot_data['county_count'] / plot_data['Population']\n",
    "county_inf_rate = plot_data[['county_en','province_en','county_inf_rate']].drop_duplicates()\n",
    "\n",
    "## Define style function which will color counties by their log inf_rate\n",
    "inf_series = county_inf_rate.set_index('county_en')['county_inf_rate']\n",
    "colorscale = branca.colormap.linear.YlOrRd_09.to_step(data = inf_series, method = 'quant', n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_function(feature):\n",
    "    county_name = feature['properties']['county_en']\n",
    "    \n",
    "    inf_rate = inf_series.get(county_name)\n",
    "    \n",
    "    if inf_rate is None:\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colorscale(inf_rate)\n",
    "        \n",
    "    return {\n",
    "        'color':'#fafafa',\n",
    "        'opactiy':0.3,\n",
    "        'fillOpacity':0.7,\n",
    "        'fillColor':fillColor,\n",
    "        'weight':0.2\n",
    "    }\n",
    "\n",
    "## Create folium choropleth map with tooltips from iran \n",
    "## geojson and our custom style function\n",
    "infrate_map_cty = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "folium.GeoJson(\n",
    "    iran_geodata,\n",
    "    name = 'iran_geodata',\n",
    "    tooltip = folium.GeoJsonTooltip(fields = ['county_en', 'province_en'], \n",
    "                                    aliases = ['County', 'Province']),\n",
    "    style_function = style_function).add_to(infrate_map_cty)\n",
    "\n",
    "infrate_map_cty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to aggregate inf_rate by province\n",
    "cases_per_prov = plot_data.groupby('province_en')[['count']].sum().reset_index()\n",
    "\n",
    "unique_prov_data = plot_data[['county_en','province_en','Population']].drop_duplicates()\n",
    "\n",
    "prov_pops = unique_prov_data.groupby('province_en').sum().reset_index()\n",
    "\n",
    "prov_inf_data = pd.merge(cases_per_prov, prov_pops)\n",
    "\n",
    "prov_inf_data['inf_rate'] = prov_inf_data['count']/prov_inf_data['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define style function which will color counties by their log inf_rate\n",
    "inf_series_prov = prov_inf_data.set_index('province_en')['inf_rate']\n",
    "colorscale = branca.colormap.linear.YlOrRd_09.to_step(data = inf_series_prov, method = 'quant', n = 100)\n",
    "\n",
    "def style_function_prov(feature):\n",
    "    prov_name = feature['properties']['province_en']\n",
    "    \n",
    "    inf_rate = inf_series_prov.get(prov_name)\n",
    "    \n",
    "    if inf_rate is None:\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colorscale(inf_rate)\n",
    "        \n",
    "    return {\n",
    "        'color':'#fafafa',\n",
    "        'opactiy':0.3,\n",
    "        'fillOpacity':0.6,\n",
    "        'fillColor':fillColor,\n",
    "        'weight':0.2\n",
    "    }\n",
    "\n",
    "infrate_map_prov = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "folium.GeoJson(\n",
    "    iran_geodata,\n",
    "    name = 'iran_geodata',\n",
    "    tooltip = folium.GeoJsonTooltip(fields = ['county_en', 'province_en'], \n",
    "                                    aliases = ['County', 'Province']),\n",
    "    style_function = style_function_prov).add_to(infrate_map_prov)\n",
    "\n",
    "infrate_map_prov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider putting prov and county maps together into one and allowing for layer control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animal Vaccination Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove null infection rates\n",
    "ani_nonulls = ani_sp_data[~ani_sp_data['animal_inf_rate'].isnull()]\n",
    "\n",
    "## Aggregate up to county level and recalculate infection rate\n",
    "ani_inf_cty = ani_nonulls.groupby('county_en')[['n_checked','n_infected']].sum()\n",
    "ani_inf_cty['inf_rate'] = ani_inf_cty['n_infected']/ani_inf_cty['n_checked']\n",
    "\n",
    "ani_series = ani_inf_cty.reset_index()[['county_en','inf_rate']].set_index('county_en')['inf_rate']\n",
    "\n",
    "## remove data entry error where more animals tested positive than were checked....\n",
    "ani_inf_rates = ani_series.drop(ani_series[ani_series > 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Initialize color scale\n",
    "colorscale = branca.colormap.linear.YlOrRd_09.to_step(data = ani_inf_rates, method = 'quant', n=100)\n",
    "colorscale\n",
    "ani_inf_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Style function for these data\n",
    "def style_function_ani(feature):\n",
    "    county_name = feature['properties']['county_en']\n",
    "    \n",
    "    inf_rate = ani_inf_rates.get(county_name)\n",
    "    \n",
    "    if inf_rate is None:\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colorscale(inf_rate)\n",
    "        \n",
    "    return {\n",
    "        'color':'#fafafa',\n",
    "        'opactiy':0.3,\n",
    "        'fillOpacity':0.7,\n",
    "        'fillColor':fillColor,\n",
    "        'weight':0.2\n",
    "    }\n",
    "\n",
    "## Create map\n",
    "ani_inf_map = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "folium.GeoJson(\n",
    "    iran_geodata,\n",
    "    name = 'iran_geodata',\n",
    "    tooltip = folium.GeoJsonTooltip(fields = ['county_en', 'province_en'], \n",
    "                                    aliases = ['County', 'Province']),\n",
    "    style_function = style_function_ani).add_to(ani_inf_map)\n",
    "\n",
    "ani_inf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove missing values and split data into non-vaccinated records and vaccinated records\n",
    "temp_data = human_sp_data[~human_sp_data['Livestock_vac_hist'].isnull()]\n",
    "temp_data = testData[temp_data['Livestock_vac_hist']!='Null']\n",
    "\n",
    "noVac_data = temp_data[temp_data['Livestock_vac_hist']=='No']\n",
    "noVac_cts = noVac_data.groupby('county_en').size().to_frame(name = 'noVac_cts')\n",
    "\n",
    "vac_data = testData[testData['Livestock_vac_hist']=='Yes']\n",
    "vac_cts = vac_data.groupby('county_en').size().to_frame(name = 'vac_cts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join aggregate no vac cases with vac cases by county\n",
    "vac_rate_data = pd.merge(noVac_cts, vac_cts, how = 'outer', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill missing values with zeros and calculate proportion vaccinated\n",
    "vac_rate_data = vac_rate_data.fillna(0)\n",
    "vac_rate_data['prop_vac'] = vac_rate_data['vac_cts']/(vac_rate_data['noVac_cts']+vac_rate_data['vac_cts'])\n",
    "vac_rates = vac_rate_data['prop_vac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize color scale\n",
    "colorscale = branca.colormap.linear.YlGn_09.to_step(data = vac_rates, method = 'quant', n=5)\n",
    "\n",
    "## Style function for these data\n",
    "def style_function_ani(feature):\n",
    "    county_name = feature['properties']['county_en']\n",
    "    \n",
    "    vac_rate = vac_rates.get(county_name)\n",
    "    \n",
    "    if vac_rate is None:\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colorscale(vac_rate)\n",
    "        \n",
    "    return {\n",
    "        'color':'#fafafa',\n",
    "        'opactiy':0.3,\n",
    "        'fillOpacity':0.7,\n",
    "        'fillColor':fillColor,\n",
    "        'weight':0.2\n",
    "    }\n",
    "\n",
    "## Create map\n",
    "ani_vac_map = folium.Map(location = [32.4279, 53.6880], \n",
    "               zoom_start = 5, \n",
    "               tiles ='cartodbpositron')\n",
    "\n",
    "folium.GeoJson(\n",
    "    iran_geodata,\n",
    "    name = 'iran_geodata',\n",
    "    tooltip = folium.GeoJsonTooltip(fields = ['county_en', 'province_en'], \n",
    "                                    aliases = ['County', 'Province']),\n",
    "    style_function = style_function_ani).add_to(ani_vac_map)\n",
    "\n",
    "ani_vac_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
