{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Reads in data, identifies county names that should be matched, updates names, joins various datasets onto the spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import Levenshtein as leven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "\n",
    "Animal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File path\n",
    "\"\"\"If we all have our folders set up the same (cloned from github) this should work for everyone.\"\"\"\n",
    "fp = os.getcwd()\n",
    "\n",
    "## Read animal data and update columns\n",
    "animal_data = pd.read_csv(os.path.join(fp, 'Data', 'animal_vac_data.csv'))\n",
    "animal_data.columns = [\n",
    "        'id', 'unitCode', 'unitType', 'province',\n",
    "        'county', 'livestock_type', 'time_j', 'time_g',\n",
    "        'lat' , 'long', 'n_sample', 'n_checked', 'n_infected', \n",
    "        'n_rejected', 'n_suspicious'\n",
    "                      ]\n",
    "\n",
    "## Create new columns storing month and year of animal testing\n",
    "animal_data[['month', 'year']] = animal_data['time_g'].str.split('/', expand = True)[[0,2]]\n",
    "\n",
    "## Sum infection data grouped by county, year, and month\n",
    "animal_data_grp = animal_data.groupby(['county', 'year', 'month'], as_index = False)[animal_data.columns[10:15]].sum().merge(animal_data)\n",
    "\n",
    "## Calculate infection rate\n",
    "animal_data_grp['animal_inf_rate'] = animal_data_grp['n_infected']/animal_data['n_sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in Iran shapefile data\n",
    "## This is what doesn't work from github...\n",
    "#iran_data = gpd.read_file(os.path.join(url, 'Iran_shp', 'iran_admin.shp'))\n",
    "iran_data = gpd.read_file(fp + '/Iran_shp/iran_admin.shp')\n",
    "\n",
    "## Subset to relevant columns and update names\n",
    "iran_data = iran_data[['ADM2_EN','ADM2_FA','ADM1_EN','ADM1_FA','Shape_Leng','Shape_Area','geometry']]\n",
    "iran_data.columns = ['county_en', 'county_fa', 'province_en', 'province_fa', 'shape_len', 'shape_area', 'geometry']\n",
    "\n",
    "## This accidental escape sequence is problematic later, so deal with manually here\n",
    "iran_data.loc[iran_data['county_en'] == 'Yasooj\\r', 'county_en'] = 'Yasooj'\n",
    "\n",
    "## Read in human data and rename columns\n",
    "human_data = pd.read_csv(os.path.join(fp, 'Data', 'Human_Brucellosis_2015-2018_V3.csv'))\n",
    "human_data = human_data.rename(columns = {'Urban/Rural/Itinerant/Nomadic':'Pop_setting',\n",
    "                                          'Prepnancy':'Pregnancy',\n",
    "                                          'Occuptio':'Occupation',\n",
    "                                          'Livestock interaction history':'Livestock_int_hist',\n",
    "                                          'Livestock interaction type':'Livestock_int_type',\n",
    "                                          'Unpasteurized dairy consumption ':'Unpast_dairy',\n",
    "                                          'Other family members infection':'Fam_members_inf',\n",
    "                                          'Outbreak Year':'Outbreak_yr',\n",
    "                                          'Outbreak Month':'Outbreak_mth',\n",
    "                                          'Diagnosis Year':'Diagnosis_yr',\n",
    "                                          'Diagnosis Month':'Diagnosis_mth',\n",
    "                                          'Livestock vaccination history':'Livestock_vac_hist'})\n",
    "\n",
    "## Fix duplicate provinces (present both capitalized and uncapitalized)\n",
    "human_data.loc[human_data['Province'] == 'Khorasan jonobi', 'Province'] = 'Khorasan Jonobi'\n",
    "human_data.loc[human_data['Province'] == 'Khorasan shomali', 'Province'] = 'Khorasan Shomali'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name spelling inconsistencies\n",
    "\n",
    "Various functions that will be helpful when trying to identify county and province name spelling discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to match potential misspelled strings\n",
    "## Takes two pandas series, calculates Levenshtein distance and ratio to identify potential matches\n",
    "## Used in the likely_matches function\n",
    "def match_names(s1, s2, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    s2 = pd.Series(s2)\n",
    "    \n",
    "    ## Unique values in each series\n",
    "    vals1 = s1.unique()\n",
    "    vals2 = s2.unique()\n",
    "    \n",
    "    ## If unique argument is set to true, only match names that don't already have perfect match\n",
    "    if unique == True:\n",
    "        \n",
    "        ## Unique values in series 1 that aren't in series 2\n",
    "        vals1 = pd.Series(np.setdiff1d(vals1, vals2))\n",
    "    \n",
    "        ## Unique values in series 2 that aren't in series 1\n",
    "        vals2 = pd.Series(np.setdiff1d(vals2, vals1))\n",
    "        \n",
    "    if caps == True:\n",
    "        \n",
    "        ## Capitalize before matching\n",
    "        vals1 = vals1.str.capitalize()\n",
    "        vals2 = vals2.str.capitalize()\n",
    "        \n",
    "    ## Calculate Levenshtein distance and ratio\n",
    "    dists = np.array([leven.distance(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "    ratios = np.array([leven.ratio(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "        \n",
    "    ## Reshape and convert to df so we can identify which values are for which name combo\n",
    "    dists_df = pd.DataFrame(data = dists.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    ratios_df = pd.DataFrame(data = ratios.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    \n",
    "    if as_df == True:\n",
    "        \n",
    "        ## Get column names where min distance and max ratio occurs\n",
    "        matches = pd.DataFrame({'name_dist': dists_df.idxmin(axis = 1), \n",
    "                                'name_ratio': ratios_df.idxmax(axis = 1), \n",
    "                                'dist': dists_df.min(axis = 1), \n",
    "                                'ratio': ratios_df.max(axis = 1)})\n",
    "    \n",
    "        return(matches)\n",
    "        \n",
    "    ## If user wants more detail, we can create a dictionary for each name that contains more graunlar info\n",
    "    ## This section creates a dictionary with names as keys and dataframes as values\n",
    "    ## Each dataframe contains all the possible name pairings (as opposed to above, which only supplies best possible values)\n",
    "    else:\n",
    "        \n",
    "        ratios_dict = {name1: ratios_df.loc[name1].sort_values(ascending = False) for name1 in vals1}\n",
    "        dists_dict = {name1: dists_df.loc[name1].sort_values() for name1 in vals1}\n",
    "        \n",
    "        comb_dict = {name1: pd.merge(dists_dict[name1], ratios_dict[name1], left_index = True, right_index = True, suffixes=('_dist', '_ratio')) for name1 in vals1}\n",
    "        \n",
    "        return(comb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to return highly probable string matches - the rest will have to be done manually\n",
    "## Depends on match_names function\n",
    "## The index of the resulting df contains values from the *first* series passed to the function\n",
    "def likely_matches(s1, s2, cutoff = 0.75, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    ## Create dataframe recording potential name matches\n",
    "    matched = match_names(s1, s2, as_df = as_df, caps = caps, unique = unique)\n",
    "    \n",
    "    ## Add column recording whether distance and ratio identify the same match\n",
    "    matched['name_match'] = (matched['name_dist'] == matched['name_ratio'])\n",
    "    \n",
    "    ## Can be highly confident when nameMatch = True, ratio >= .75 - this matches 145 of the 192 that need matches\n",
    "    matched['matched'] = np.where((matched['name_match'] == True) & (matched['ratio'] >= cutoff), matched['name_dist'], 'NULL')\n",
    "           \n",
    "    return(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Little helper function that creates a dictionary mapping capitalized values\n",
    "## to original values (so we can go back and forth more easily)\n",
    "def map_caps(s1):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    \n",
    "    ## Capitalize series values\n",
    "    s1_caps = s1.str.capitalize().unique()\n",
    "    \n",
    "    ## Map original values to capitalized values\n",
    "    caps_mappings = {name1:name2 for name1, name2 in zip(s1_caps, s1.unique())}\n",
    "    \n",
    "    return(caps_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Province Matching ##\n",
    "provs1 = human_data.loc[human_data['Province'] != 'Null']['Province']\n",
    "provs2 = iran_data['province_en']\n",
    "\n",
    "#likely_matches(provs1, provs2, caps = False)\n",
    "#test = match_names(provs1, provs2, as_df = False, caps = False, unique = True)\n",
    "\n",
    "## Province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'Azarbaijan Gharbi',\n",
    "    'East Azerbaijan':'Azarbaijan Sharghi',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh & Boyerahmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan'\n",
    "              }\n",
    "\n",
    "## Invert dictionary\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "human_data['Province'] = human_data['Province'].map(match_dict_prov).fillna(human_data['Province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County Matching\n",
    "## Remove null values for matching\n",
    "counties1 = human_data.loc[human_data['County'] != 'Null']['County']\n",
    "counties2 = iran_data['county_en']\n",
    "\n",
    "## Create mapping of likely pairs\n",
    "matched_df = likely_matches(counties1, counties2)\n",
    "#match_names(counties1, counties2, as_df = False)\n",
    "\n",
    "#matched_df[matched_df['matched'] == 'NULL']\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "\n",
    "## Revert automatched names back to original form data files and zip matched pairs into dictionary\n",
    "caps_mappings1 = map_caps(counties1)\n",
    "caps_mappings2 = map_caps(counties2)\n",
    "\n",
    "match_dict_cty = dict(zip(automatched.index.map(caps_mappings1), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manual matching\n",
    "match_dict_man = {\n",
    "    'Ali Abad Katul':'Aliabad', \n",
    "    'Bafgh':'Bafq', \n",
    "    'Bandar Qaz':'Bandar-e-Gaz', \n",
    "    'Dailam':'Deylam',\n",
    "    'Gonbad  kavoos':'Gonbad-e-Kavus', \n",
    "    'Ijroud':'Eejrud', \n",
    "    'Jovein':'Jowayin',\n",
    "    'Kalale':'Kolaleh',\n",
    "    'Mahvalat':'Mahvelat', \n",
    "    'Menojan':'Manujan', ## auto-matched incorrectly\n",
    "    'Neyshabur':'Nishapur',  \n",
    "    'Orzoieyeh':'Arzuiyeh', \n",
    "    'Ray':'Rey', \n",
    "    'Tehran Jonub':'Tehran', \n",
    "    'Tehran Shomal':'Tehran', \n",
    "    'Tiran o Karvan':'Tiran-o-Korun',\n",
    "    'Abadeh Tashk':'Abadeh',\n",
    "    'Agh Ghala':'Aqqala',\n",
    "    'Ahvaz e gharb':'Ahvaz',\n",
    "    'Ahvaz e Shargh':'Ahvaz',\n",
    "    'Gilan Qarb':'Gilan-e-Gharb',\n",
    "    'Kharame':'Kherameh',\n",
    "    'Maraqe':'Maragheh',\n",
    "    'Tehran Gharb':'Tehran',\n",
    "    'Tehran Shargh':'Tehran',\n",
    "    'Tehran Shomal Qarb':'Tehran',\n",
    "    'Zaveh':'Zave',\n",
    "    'Bandar Mahshahr':'Mahshahr',\n",
    "    'Qale ganj':'Ghaleye-Ganj',\n",
    "    'Sarchahan':'Hajiabad',\n",
    "    'Kamfirouz':'Marvdasht',\n",
    "    'Zarghan': 'Shiraz',\n",
    "    'Beyza':'Sepidan',\n",
    "    'Dore Chagni':'Doureh',\n",
    "    'Sepid Dasht':'Khorramabad',\n",
    "    'Nour Abad':'Mamasani',\n",
    "    'Aleshtar':'Selseleh',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Saduq':'Yazd',\n",
    "    'Mashhad Morghab':'Khorrambid',\n",
    "    'Dehdez':'Izeh',\n",
    "    'zaboli':'Mehrestan',\n",
    "    'Qaemiyeh':'Kazerun',\n",
    "    'Samen ol Aemmeh':'Mashhad',\n",
    "    'kish':'Bandar-Lengeh'\n",
    "              }\n",
    "\n",
    "match_dict_cty.update(match_dict_man) ## This now has all automatched names and manually matched names\n",
    "\n",
    "## Add all the perfect matches to this dictionary\n",
    "## Mapping dictionary now should include all matches\n",
    "perf_matches = np.intersect1d(iran_data['county_en'].str.capitalize(), human_data['County'].str.capitalize())\n",
    "\n",
    "match_dict_cty.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Map names in dataframe based on dictionary\n",
    "human_data['County'] = human_data['County'].map(match_dict_cty).fillna(human_data['County'])\n",
    "\n",
    "## Joining ##\n",
    "human_sp_data = pd.merge(human_data, iran_data, how = 'outer', left_on = 'County', right_on = 'county_en')\n",
    "\n",
    "## Write mapping dictionary to csv for ease of QA\n",
    "# pd.DataFrame.from_dict(data=match_dict_cty, orient='index').to_csv(fp + '/human_data_mappings.csv', index_label = ['human_county'], header=['shp_county'])\n",
    "\n",
    "## QUALITY ASSURANCE NOTES ##\n",
    "# 'Behbahan' associated with 2 provinces in the human data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Province matching ##\n",
    "\n",
    "#provs1 = animal_data['province']\n",
    "#provs2 = iran_data['province_en']\n",
    "\n",
    "#likely_matches(provs1, provs2, caps = False)\n",
    "#match_names(provs1, provs2, as_df = True, caps = False, unique = True)\n",
    "\n",
    "## Province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'West Azarbayjan',\n",
    "    'East Azerbaijan':'East Azarbayjan',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan',\n",
    "    'Hamadan':'Hamedan',\n",
    "    'Kermanshan':'Kermanshah',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh and BoyerAhmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Kerman':'South Kerman'\n",
    "              }\n",
    "\n",
    "## Invert dictionary\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "animal_data['province'] = animal_data['province'].map(match_dict_prov).fillna(animal_data['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County names ##\n",
    "ani_cnties = animal_data['county']\n",
    "\n",
    "matched_df = likely_matches(ani_cnties, iran_data['county_en'])\n",
    "#match_names(ani_cnties, counties2, as_df = False)\n",
    "\n",
    "ani_caps_mappings = map_caps(ani_cnties)\n",
    "\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "#unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start mapping dictionary with the automatched names\n",
    "match_dict_ani = dict(zip(automatched.index.map(ani_caps_mappings), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_ani = {\n",
    "    'Aran and Bidgol':'Aran-o-Bidgol',\n",
    "    'Buin and Miandasht':'Booeino Miyandasht',\n",
    "    'Deyr':'Dayyer',\n",
    "    'Haftkel':'Haftgol',\n",
    "    'Ijrud':'Eejrud',\n",
    "    'Maneh asd Samalgan':'Maneh-o-Samalqan',\n",
    "    'Orzueeyeh':'Arzuiyeh',\n",
    "    'Qaleh Ganj':'Ghaleye-Ganj',\n",
    "    'Qir and Karzin':'Qir-o-Karzin',\n",
    "    'Raz and Jargalan':'Razo Jalgelan',\n",
    "    'Sib and Suran':'Sibo Soran',\n",
    "    'Tiran and Karvan':'Tiran-o-Korun',\n",
    "    'Torqebeh and Shandiz(Binalud)':'Torghabe-o-Shandiz',\n",
    "    'Zaveh':'Zave',\n",
    "    'Chardavol':'Shirvan-o-Chardavol',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'mahshahr':'Mahshahr',\n",
    "    'Jafarieh':'Torbat-e-Jam',\n",
    "    'Kahak':'Sabzevar',\n",
    "    'Kohgiluyeh and BoyerAhmad':'Kohgeluyeh',\n",
    "    'BoyerAhmad':'Yasooj'\n",
    "              }\n",
    "\n",
    "match_dict_ani.update(match_dict_man_ani)\n",
    "\n",
    "#unmatched2 = [name for name in unmatched.index.map(ani_caps_mappings) if not name in match_dict_ani.keys()]\n",
    "\n",
    "## Add identical matches to the dictionary\n",
    "perf_matches = np.intersect1d(iran_data['county_en'], animal_data['county'])\n",
    "match_dict_ani.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Update county names in animal data and do the join\n",
    "animal_data['county'] = animal_data['county'].map(match_dict_ani).fillna(animal_data['county'])\n",
    "ani_sp_data = pd.merge(animal_data, iran_data, how = 'outer', left_on = 'county', right_on = 'county_en')\n",
    "\n",
    "## Write mapping dictionary to csv for ease of QA\n",
    "# pd.DataFrame.from_dict(data=match_dict_ani, orient='index').to_csv(fp + '/animal_data_mappings.csv', index_label = ['animal_county'], header = ['shp_county'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SES data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in SES data and rename columns\n",
    "ses_data = pd.read_csv(os.path.join(fp, 'Data', 'ses_data.csv'))[['province', 'pop', 'hshld_size', 'ses']]\n",
    "\n",
    "## Get province names\n",
    "ses_provs = ses_data['province'].unique()\n",
    "\n",
    "## Automatch ses names to spatial data province names\n",
    "ses_matches = likely_matches(ses_provs, iran_data['province_en'].unique())\n",
    "\n",
    "## Maps from matched names back to uncapitalized names\n",
    "ses_caps_map = map_caps(ses_provs)\n",
    "iran_prov_map = map_caps(iran_data['province_en'].unique())\n",
    "\n",
    "## Determine names that were successfully matched\n",
    "automatched = ses_matches[ses_matches['matched'] != 'NULL']\n",
    "\n",
    "## Create dictionary mapping ses names to spatial data names and update ses_data\n",
    "match_dict_ses = dict(zip(automatched.index.map(ses_caps_map), automatched['matched'].map(iran_prov_map)))\n",
    "ses_data['province'] = ses_data['province'].map(match_dict_ses).fillna(ses_data['province'])\n",
    "\n",
    "## Join data\n",
    "ses_sp_data = pd.merge(ses_data, iran_data, how = 'outer', left_on = 'province', right_on = 'province_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County population data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in population data and clean strings\n",
    "pop_data = pd.read_csv(os.path.join(fp, 'Data', 'pop_by_county.csv'), skiprows = [1, 2, 3, 4], usecols = [0, 1])\n",
    "\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Setteled\", case = False)].index)\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Settled\", case = False)].index)\n",
    "\n",
    "pop_data['Description'] = pop_data['Description'].str.strip()\n",
    "pop_data['Population'] = pop_data['Population'].str.replace(',', '').astype(int)\n",
    "\n",
    "## Need to append provinces and counties from iran_data because the pop data is not separated by prov/county\n",
    "provs = iran_data['province_en']\n",
    "cts = iran_data['county_en']\n",
    "\n",
    "all_names = provs.append(cts)\n",
    "\n",
    "## Auto-match names\n",
    "matched_df = likely_matches(pop_data['Description'], all_names)\n",
    "\n",
    "## Map original data names to capitalized names\n",
    "pop_caps_mappings = map_caps(pop_data['Description'])\n",
    "iran_caps_mappings = map_caps(all_names)\n",
    "\n",
    "## Identify automatched and unmatched names\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start match dictionary with automatched names\n",
    "match_dict_pop = dict(zip(automatched.index.map(pop_caps_mappings), automatched['matched'].map(iran_caps_mappings)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_pop = {\n",
    "    'Arzooeyeh':'Arzuiyeh',\n",
    "    'Bafgh':'Bafq',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Firooze':'Firuzeh',\n",
    "    'Ijerud':'Eejrud',\n",
    "    'Ivan':'Eyvan',\n",
    "    'Jovin':'Jowayin',\n",
    "    'Mayamee':'Meyami',\n",
    "    'Naeen':'Nain',\n",
    "    'Neemrooz':'Nimrouz',\n",
    "    'Neyshabur':'Nishapur',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'Zaveh':'Zave',\n",
    "    'Khorasan-e-Razavi':'Razavi Khorasan',\n",
    "    'Qaleh-Ganj':'Ghaleye-Ganj',\n",
    "    'Qaser-e Qand':'Ghasre Ghand',\n",
    "    'Raz & Jargalan':'Razo Jalgelan',\n",
    "    'Reegan':'Rigan',\n",
    "    'Savadkuh-e Shomali':'Savadkuh',\n",
    "    'Sireek':'Sirik',\n",
    "    'Sumaehsara':'Some\\'e-Sara',\n",
    "    'Tiran & Karvan':'Tiran-o-Korun',\n",
    "    'Zeerkooh':'Zirkouh',\n",
    "    'Bandar-e-Mahshahr':'Mahshahr',\n",
    "    'Bon':'Ben',\n",
    "    'Chardavel':'Shirvan-o-Chardavol',\n",
    "    'Fonuch':'Fanouj',\n",
    "    'Keyar':'Kiaar',\n",
    "    'Qayenat':'Qaen',\n",
    "    'Qods':'Shahr-e Qods',\n",
    "    'Sibsavaran':'Sibo Soran',\n",
    "    'Kordestan':'Kurdistan'\n",
    "              }\n",
    "\n",
    "## Update dictionary with manual matches\n",
    "match_dict_pop.update(match_dict_man_pop)\n",
    "\n",
    "## Still unmatched:\n",
    "#unmatched2 = [name for name in unmatched.index.map(pop_caps_mappings) if not name in match_dict_pop.keys()]\n",
    "\n",
    "## Add identical matches to dictionary\n",
    "perf_matches = np.intersect1d(all_names, pop_data['Description'])\n",
    "match_dict_pop.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "pop_data['Mapped'] = pop_data['Description'].map(match_dict_pop)\n",
    "\n",
    "## Write mappings to file for reference\n",
    "#pd.DataFrame.from_dict(data=match_dict_pop, orient='index').to_csv(fp + '/pop_data_mappings.csv', index_label = ['pop_county'], header = ['shp_county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disentangling instances where provinces and county names match (since they're all lumped together in this case):\n",
    "## Identify duplicate names\n",
    "dup_names = pop_data[pd.DataFrame.duplicated(pop_data, 'Description')][['Description','Mapped']]\n",
    "\n",
    "## Sort by population. We know that the first entry for each name will be the province pop\n",
    "## and the second entry will be the county prop since prov pop >= county pop\n",
    "dup_vals = pop_data[pop_data['Description'].isin(dup_names['Description'])].sort_values(by = ['Description','Population'], ascending=[True, False])\n",
    "dup_vals['Geog_region'] = np.resize(['Province','County'], len(dup_vals))\n",
    "\n",
    "## Tagging each name as either province or county\n",
    "dup_pop_data_provs = dup_vals[dup_vals['Geog_region'] == 'Province']\n",
    "nondup_pop_data_provs = pop_data[pop_data['Mapped'].isin(provs.sort_values().unique()) & (~pop_data['Description'].isin(dup_pop_data_provs['Description']))]\n",
    "nondup_pop_data_provs['Geog_region'] = 'Province'\n",
    "\n",
    "## Merge back on population data - now everything is tagged to indicate province or county\n",
    "merge1 = pd.merge(nondup_pop_data_provs, dup_vals, how='outer')\n",
    "merge2 = pd.merge(merge1, pop_data, how='outer')\n",
    "\n",
    "## Drop provinces - we only want counties\n",
    "pop_data_cts_only = merge2[merge2['Geog_region']!='Province'][['Mapped','Population']]\n",
    "\n",
    "## Merge with spatial data on county name\n",
    "pop_sp_data = pd.merge(pop_data_cts_only, iran_data, how = 'outer', left_on = 'Mapped', right_on = 'county_en')\n",
    "\n",
    "## Drop erroneous row - results from original pop_data file having this entry listed twice.\n",
    "pop_sp_data = pop_sp_data[pop_sp_data['Mapped']!='Razavi Khorasan']\n",
    "\n",
    "## Need to identify the last few matches - a couple iran_data counties not matched:\n",
    "## pop_data has unmatched: \"Binalood\" and \"Nayer\"\n",
    "## Iran data has unmatched: Khusf, Nir, Northern Savadkooh, Torghabe-o-Shandiz, Urumia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to add environmental data to spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This may rely on one or two manual edits to the data depending on which source you use\n",
    "#(some values are float type nan which causes issues)\n",
    "#so feel free to comment this cell out if it's giving an error.\n",
    "\n",
    "import jdatetime\n",
    "\n",
    "def addGregorian(data, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Adds gregorian month and year columns to a dataframe based on jalali month and year columns.\n",
    "    Assumes first day of jalali month.\n",
    "    \"\"\"\n",
    "    #Apply function iterates a function over each row in a dataframe (axis=1 specifies row)\n",
    "    #The function creates a jalali date object for each row from the jalali year and month. \n",
    "    #Then, it converts it to a gregorian date and extracts the year or month. \n",
    "    #If the jalali year and date are not numeric (e.g. Null as a string) then the year and month get None values. \n",
    "    data['year']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian().strftime('%y') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)\n",
    "    data['month']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian().strftime('%m') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)\n",
    "\n",
    "def addEnvData(data, envDF, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Gets environmental variables from a dataframe containing them for each row in a dataframe based on county and date.\n",
    "    \"\"\"\n",
    "    #Apply function iterates a function over each row in a dataframe (axis=1 specifies row)\n",
    "    #envDF.loc[row.County] sorts the environmental data to the correct county. \n",
    "    #The next bracket selects a column by creating a datestring in the correct format. .zfill is required to make sure months are in '04' format instead of '4'\n",
    "    data['mean_2m_air_temperature'] = data.apply(lambda row: envDF.loc[row.County]['mean_2m_air_temperature_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_total_precipitation'] = data.apply(lambda row: envDF.loc[row.County]['total_precipitation_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_ndvi'] = data.apply(lambda row: envDF.loc[row.County]['mean_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_elevation'] = data.apply(lambda row: envDF.loc[row.County]['mean_elevation'] if (row.County in envDF.index) else None, axis=1)\n",
    "\n",
    "## Read Environmental Data\n",
    "envFP=os.path.join(fp, 'Data', 'allParams.csv')\n",
    "envData = pd.read_csv(envFP, index_col='ADM2_EN')\n",
    "\n",
    "\n",
    "## Clean up human data, add date column\n",
    "human_sp_data.loc[pd.isna(human_sp_data['Outbreak_yr']), 'Outbreak_yr']='Null'\n",
    "addGregorian(human_sp_data, 'Outbreak_yr', 'Outbreak_mth')\n",
    "\n",
    "## Clean up animal data\n",
    "ani_sp_data.rename(columns={\"county\": \"County\"}, inplace=True)\n",
    "\n",
    "#addEnvData(human_sp_data, envData, 'year', 'month')\n",
    "addEnvData(ani_sp_data, envData, 'year', 'month')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
