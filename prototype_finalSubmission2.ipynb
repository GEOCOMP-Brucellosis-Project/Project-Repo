{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bovine Brucellosis\n",
    "\n",
    "A spatio-temporal analysis of the relationship between human and animal Brucellosis cases, focusing on the association between animal vaccination practices and disease transmission by county in Iran.\n",
    "\n",
    "This project seeks to investigate the relationship between disease incidence in humans with various factors at the county level. We are considering animal vaccination status expressed as percent vaccinated out of total population in county, socioeconomic (SES) variables, and environmental factors including precipitation, temperature, NDVI, and elevation to see if they have any affect.\n",
    "\n",
    "## Workflow and Current Status\n",
    "1. **Collect animal, human data, and socioeconomic data <-- Completed**\n",
    "1. **Analyze data and develop project direction/scope <-- Completed**\n",
    "1. **Collect environmental data using Google Earth Engine <-- Completed**\n",
    "1. **Clean and merge all data <-- Completed**\n",
    "1. Investigate trends in human incidence through regression/multiple regression models <--In Progress\n",
    "\n",
    "Validating regression models is a crucial next step, though likely outside the scope of this semester.\n",
    "\n",
    "In this document you will find a working prototype of our project code so far. The code cleans the human, animal, SES, and environmental datasets, and merges them into one. It also investigates spatial clustering trends in the human and animal data through Moran's i. \n",
    "\n",
    "The code for summarizing each environmental variable by county is found in a separate jupyter notebook called \"EarthEngineCode_GoogleCollab.ipynb\". To run this code, either run the notebook in a Google Collab Notebook (preferred) or install the earthengine module into your local python environment. You will need an EarthEngine account to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning\n",
    "\n",
    "Our project requires integrating information from the following data sources:\n",
    "\n",
    "- Human data\n",
    "    - Data on human brucellosis cases in Iran from 2015 to 2018\n",
    "    - Contains demographic information on each case as well as information on past interaction with livestock, livestock vaccination status, and any known link to unpasteurized dairy consumption\n",
    "- Animal data\n",
    "    - Data on brucellosis incidence in livestock in Iran from 2009 to 2017\n",
    "    - Contains results from sampling of barns across the country (includes number of animals sampled, number infected, number healthy, and number of suspicious cases)\n",
    "- Socioeconomic status data\n",
    "    - Data containing average household size and socioeconomic status quintile at the province level\n",
    "- Population data\n",
    "    - Contains state-reported population data for each county\n",
    "- Environmental data\n",
    "    - NDVI, temperature, humidity, and precipitation data obtained using Google Earth Engine\n",
    "- Spatial data\n",
    "    - Shapefile of Iranian counties\n",
    "    \n",
    "In the code sections below, we read in each of these datasets in turn (environmental data is saved for later in the script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import Levenshtein as leven\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights.contiguity import Queen\n",
    "import splot\n",
    "from esda.moran import Moran\n",
    "from splot.esda import moran_scatterplot\n",
    "from splot.esda import plot_moran\n",
    "from splot.esda import moran_scatterplot\n",
    "from esda.moran import Moran_Local\n",
    "from splot.esda import lisa_cluster\n",
    "from splot.esda import plot_local_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File path\n",
    "fp = os.getcwd()\n",
    "os.path.join(fp, 'Data')\n",
    "\n",
    "## Read animal data and update columns\n",
    "animal_data = pd.read_csv(os.path.join(fp, 'Data', 'animal_vac_data.csv'))\n",
    "animal_data.columns = ['id', 'unitCode', 'unitType', 'province',\n",
    "                       'county', 'livestock_type', 'time_j', 'time_g',\n",
    "                       'lat' , 'long', 'n_sample', 'n_checked', 'n_infected', \n",
    "                       'n_rejected', 'n_suspicious']\n",
    "\n",
    "## Create new columns storing month and year of animal testing\n",
    "animal_data[['month', 'year']] = animal_data['time_g'].str.split('/', expand = True)[[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapefile data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in Iran shapefile data\n",
    "iran_data = gpd.read_file(os.path.join(fp, 'Iran_shp/iran_admin.shp'))\n",
    "\n",
    "## Subset to relevant columns and update names\n",
    "iran_data = iran_data[['ADM2_EN','ADM2_FA','ADM1_EN','ADM1_FA','Shape_Leng','Shape_Area','geometry']]\n",
    "iran_data.columns = ['county_en', 'county_fa', 'province_en', 'province_fa', 'shape_len', 'shape_area', 'geometry']\n",
    "\n",
    "## This accidental escape sequence is problematic later, so deal with manually here\n",
    "iran_data.loc[iran_data['county_en'] == 'Yasooj\\r', 'county_en'] = 'Yasooj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in human data and rename columns\n",
    "human_data = pd.read_csv(os.path.join(fp, 'Data', 'Human_Brucellosis_2015-2018_V3.csv'))\n",
    "human_data = human_data.rename(columns = {'Urban/Rural/Itinerant/Nomadic':'Pop_setting',\n",
    "                                          'Prepnancy':'Pregnancy',\n",
    "                                          'Occuptio':'Occupation',\n",
    "                                          'Livestock interaction history':'Livestock_int_hist',\n",
    "                                          'Livestock interaction type':'Livestock_int_type',\n",
    "                                          'Unpasteurized dairy consumption ':'Unpast_dairy',\n",
    "                                          'Other family members infection':'Fam_members_inf',\n",
    "                                          'Outbreak Year':'Outbreak_yr',\n",
    "                                          'Outbreak Month':'Outbreak_mth',\n",
    "                                          'Diagnosis Year':'Diagnosis_yr',\n",
    "                                          'Diagnosis Month':'Diagnosis_mth',\n",
    "                                          'Livestock vaccination history':'Livestock_vac_hist'})\n",
    "\n",
    "## Fix duplicate provinces (present both capitalized and uncapitalized)\n",
    "human_data.loc[human_data['Province'] == 'Khorasan jonobi', 'Province'] = 'Khorasan Jonobi'\n",
    "human_data.loc[human_data['Province'] == 'Khorasan shomali', 'Province'] = 'Khorasan Shomali'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socioeconomic status data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in SES data and rename columns\n",
    "ses_data = pd.read_csv(os.path.join(fp, 'Data','ses_data.csv'))[['province', 'pop', 'hshld_size', 'ses']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County-level population data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in population data\n",
    "pop_data = pd.read_csv(os.path.join(fp, 'Data', 'pop_by_county.csv'), skiprows = [1, 2, 3, 4], usecols = [0, 1])\n",
    "\n",
    "## Remove unneeded rows\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Setteled\", case = False)].index)\n",
    "pop_data = pop_data.drop(pop_data[pop_data['Description'].str.contains(\"Settled\", case = False)].index)\n",
    "\n",
    "## Trim strings\n",
    "pop_data['Description'] = pop_data['Description'].str.strip()\n",
    "pop_data['Population'] = pop_data['Population'].str.replace(',', '').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Name Discrepancies\n",
    "\n",
    "Of course, working with data from a variety of sources requires extensive data preparation and cleaning to ensure the correct observations are matched when merging the data into a single source.\n",
    "\n",
    "We discovered quickly that translations of province and county names from Persian to English varied widely across the datasets. This presented serious problems for our ability to merge datasets, as even single character differences between county names across the datasets would prevent the data from being matched properly. However, given the more than 400 distinct counties in Iran, it was impractical to attempt to manually identify the discrepancies and update the names accordingly. To speed up our ability to identify county name discrepancies, we defined three functions to help identify many of the matches automatically:\n",
    "\n",
    "- match_names\n",
    "- likely_matches\n",
    "- map_caps\n",
    "\n",
    "### Function: match_names\n",
    "\n",
    "match_names is a function that iterates through two user-supplied lists of strings and calculates the Levenshtein distance and Levenshtein ratio between the distinct pairings of strings between the two lists. It then assembles a dataframe displaying the matches that each method (Levenshtein distance or Levenshtein ratio) identifies as the best possible match. The function will find a potential match for all strings in the first list, but will not necessarily find a match for all strings in the second list passed.\n",
    "\n",
    "The Levenshtein distance and Levenshtein ratio are string similarity metrics that rely on the number of individual character changes are required to convert one string into another. More information can be found [here](https://en.wikipedia.org/wiki/Levenshtein_distance).\n",
    "\n",
    "There are a few options for the user to specify:\n",
    "\n",
    "- __as_df:__ if True, returns a single dataframe with the automatically identified optimum matches. If False, returns a dictionary of where each string in the first list is represented as a key. The associated values are dataframes of all the Levenshtein distances and ratios for that key string when matched to each word in the second list. (This allows for more granular investigation when the top match seems questionable.)\n",
    "- __caps:__ if True, standardizes capitalization of all strings before matching. May improve match success depending on the format of the strings passed to the function\n",
    "- __unique:__ if True, the function only attempts to match names that don't already have a perfect match. This usually improves accuracy since it removes the possiblity of matching a string in the first list to a value that already has a perfect match to another string in the second list. However, when a one-to-one matching isn't expected, setting this to False may improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to match potential misspelled strings\n",
    "## Takes two lists, calculates Levenshtein distance and ratio to identify potential matches\n",
    "## Used in the likely_matches function\n",
    "def match_names(s1, s2, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    s2 = pd.Series(s2)\n",
    "    \n",
    "    ## Get unique values in each series\n",
    "    vals1 = s1.unique()\n",
    "    vals2 = s2.unique()\n",
    "    \n",
    "    ## If unique argument is set to true, only match names that don't already have perfect match\n",
    "    if unique == True:\n",
    "        \n",
    "        ## Unique values in series 1 that aren't in series 2\n",
    "        vals1 = pd.Series(np.setdiff1d(vals1, vals2))\n",
    "    \n",
    "        ## Unique values in series 2 that aren't in series 1\n",
    "        vals2 = pd.Series(np.setdiff1d(vals2, vals1))\n",
    "        \n",
    "    if caps == True:\n",
    "        \n",
    "        ## Capitalize before matching\n",
    "        vals1 = vals1.str.capitalize()\n",
    "        vals2 = vals2.str.capitalize()\n",
    "        \n",
    "    ## Calculate Levenshtein distance and ratio\n",
    "    dists = np.array([leven.distance(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "    ratios = np.array([leven.ratio(name1, name2) for name1 in vals1 for name2 in vals2])\n",
    "        \n",
    "    ## Reshape and convert to df so we can identify which values are for which name combo\n",
    "    dists_df = pd.DataFrame(data = dists.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    ratios_df = pd.DataFrame(data = ratios.reshape(len(vals1), len(vals2)), index = vals1, columns = vals2)\n",
    "    \n",
    "    if as_df == True:\n",
    "        \n",
    "        ## Get column names where min distance and max ratio occurs\n",
    "        matches = pd.DataFrame({'name_dist': dists_df.idxmin(axis = 1), \n",
    "                                'name_ratio': ratios_df.idxmax(axis = 1), \n",
    "                                'dist': dists_df.min(axis = 1), \n",
    "                                'ratio': ratios_df.max(axis = 1)})\n",
    "    \n",
    "        return(matches)\n",
    "        \n",
    "    ## If user wants more detail, we can create a dictionary for each name that contains more graunlar info\n",
    "    ## This section creates a dictionary with names as keys and dataframes as values\n",
    "    ## Each dataframe contains all the possible name pairings (as opposed to above, which only supplies best possible values)\n",
    "    else:\n",
    "        \n",
    "        ratios_dict = {name1: ratios_df.loc[name1].sort_values(ascending = False) for name1 in vals1}\n",
    "        dists_dict = {name1: dists_df.loc[name1].sort_values() for name1 in vals1}\n",
    "        \n",
    "        comb_dict = {name1: pd.merge(dists_dict[name1], ratios_dict[name1], left_index = True, right_index = True, suffixes=('_dist', '_ratio')) for name1 in vals1}\n",
    "        \n",
    "        return(comb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: likely_matches\n",
    "\n",
    "likely_matches uses the match_names function to identify potential name matches, but also checks to determine whether the optimum match identified by the Levenshtein distance method and Levenshtein ratio method are the same. If both of these guesses are the same and the ratio is sufficiently high, the function populates a dataframe identifying those matches in a new column. This allows the user to easily identify which strings had accurate matches and which remain unmatched (either because the Levenshtein ratio is not sufficiently high or the two methods disagree about the best match).\n",
    "\n",
    "In addition to the arguments that can be specified in match_names, likely_matches also allows the user to specify the ratio cutoff that will be used to classify strings as matched or unmatched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to return highly probable string matches - the rest will have to be done manually\n",
    "## Depends on match_names function\n",
    "## The index of the resulting df contains values from the *first* series passed to the function\n",
    "def likely_matches(s1, s2, cutoff = 0.75, as_df = True, caps = True, unique = True):\n",
    "    \n",
    "    ## Create dataframe recording potential name matches\n",
    "    matched = match_names(s1, s2, as_df = as_df, caps = caps, unique = unique)\n",
    "    \n",
    "    ## Add column recording whether distance and ratio identify the same match\n",
    "    matched['name_match'] = (matched['name_dist'] == matched['name_ratio'])\n",
    "    \n",
    "    ## Can be highly confident when nameMatch = True, ratio >= .75 - this matches 145 of the 192 that need matches\n",
    "    matched['matched'] = np.where((matched['name_match'] == True) & (matched['ratio'] >= cutoff), matched['name_dist'], 'NULL')\n",
    "           \n",
    "    return(matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: map_caps\n",
    "\n",
    "This is a small helper function to be used in conjunction with match_names and likely_matches. When caps is set to True in either of those functions, strings are capitalized before matching. We found that this typically improves matching accuracy, but it is important to be able to revert the strings back to their original form before the capitalization. \n",
    "\n",
    "map_caps simply creates a dictionary mapping a list of strings to their capitalized values. Then, regardless of whether match_names and likely_matches are used with capitalization or not, the user can easily find the original string values before the matching process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Little helper function that creates a dictionary mapping capitalized values\n",
    "## to original values (so we can go back and forth more easily)\n",
    "def map_caps(s1):\n",
    "    \n",
    "    s1 = pd.Series(s1)\n",
    "    \n",
    "    ## Capitalize series values\n",
    "    s1_caps = s1.str.capitalize().unique()\n",
    "    \n",
    "    ## Map original values to capitalized values\n",
    "    caps_mappings = {name1:name2 for name1, name2 in zip(s1_caps, s1.unique())}\n",
    "    \n",
    "    return(caps_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Province and County Name Matching\n",
    "\n",
    "We split the process of matching names for each dataset into province names and county names. For the human data, the likely_matches function only auto-identified 4 matches, but given that all but 10 provinces were consistent across the human data and iran shapefile at the beginning, it was simple to manually match the remaining 6 names. We created a dictionary to store the mappings from the human data province names to their shapefile counterparts and updated the human data dataframe with the corresponding matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_dist</th>\n",
       "      <th>name_ratio</th>\n",
       "      <th>dist</th>\n",
       "      <th>ratio</th>\n",
       "      <th>name_match</th>\n",
       "      <th>matched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Azarbaijan Gharbi</th>\n",
       "      <td>Kermanshah</td>\n",
       "      <td>East Azerbaijan</td>\n",
       "      <td>12</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Azarbaijan Sharghi</th>\n",
       "      <td>Kermanshah</td>\n",
       "      <td>East Azerbaijan</td>\n",
       "      <td>12</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chaharmahal &amp; bakhtiari</th>\n",
       "      <td>Chaharmahal and Bakhtiari</td>\n",
       "      <td>Chaharmahal and Bakhtiari</td>\n",
       "      <td>4</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>True</td>\n",
       "      <td>Chaharmahal and Bakhtiari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Esfahan</th>\n",
       "      <td>Isfahan</td>\n",
       "      <td>Isfahan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>True</td>\n",
       "      <td>Isfahan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Khorasan Jonobi</th>\n",
       "      <td>Kerman</td>\n",
       "      <td>North Khorasan</td>\n",
       "      <td>11</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Khorasan Razavi</th>\n",
       "      <td>Kermanshah</td>\n",
       "      <td>North Khorasan</td>\n",
       "      <td>10</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Khorasan Shomali</th>\n",
       "      <td>Kermanshah</td>\n",
       "      <td>North Khorasan</td>\n",
       "      <td>10</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kohgiluyeh &amp; Boyerahmad</th>\n",
       "      <td>Kohgiluyeh and Boyer-Ahmad</td>\n",
       "      <td>Kohgiluyeh and Boyer-Ahmad</td>\n",
       "      <td>5</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>True</td>\n",
       "      <td>Kohgiluyeh and Boyer-Ahmad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kordestan</th>\n",
       "      <td>Kurdistan</td>\n",
       "      <td>Lorestan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>False</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sistan &amp; Bluchestan</th>\n",
       "      <td>Sistan and Baluchestan</td>\n",
       "      <td>Sistan and Baluchestan</td>\n",
       "      <td>4</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>True</td>\n",
       "      <td>Sistan and Baluchestan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name_dist  \\\n",
       "Azarbaijan Gharbi                        Kermanshah   \n",
       "Azarbaijan Sharghi                       Kermanshah   \n",
       "Chaharmahal & bakhtiari   Chaharmahal and Bakhtiari   \n",
       "Esfahan                                     Isfahan   \n",
       "Khorasan Jonobi                              Kerman   \n",
       "Khorasan Razavi                          Kermanshah   \n",
       "Khorasan Shomali                         Kermanshah   \n",
       "Kohgiluyeh & Boyerahmad  Kohgiluyeh and Boyer-Ahmad   \n",
       "Kordestan                                 Kurdistan   \n",
       "Sistan & Bluchestan          Sistan and Baluchestan   \n",
       "\n",
       "                                         name_ratio  dist     ratio  \\\n",
       "Azarbaijan Gharbi                   East Azerbaijan    12  0.562500   \n",
       "Azarbaijan Sharghi                  East Azerbaijan    12  0.545455   \n",
       "Chaharmahal & bakhtiari   Chaharmahal and Bakhtiari     4  0.875000   \n",
       "Esfahan                                     Isfahan     1  0.857143   \n",
       "Khorasan Jonobi                      North Khorasan    11  0.551724   \n",
       "Khorasan Razavi                      North Khorasan    10  0.551724   \n",
       "Khorasan Shomali                     North Khorasan    10  0.533333   \n",
       "Kohgiluyeh & Boyerahmad  Kohgiluyeh and Boyer-Ahmad     5  0.857143   \n",
       "Kordestan                                  Lorestan     2  0.823529   \n",
       "Sistan & Bluchestan          Sistan and Baluchestan     4  0.878049   \n",
       "\n",
       "                         name_match                     matched  \n",
       "Azarbaijan Gharbi             False                        NULL  \n",
       "Azarbaijan Sharghi            False                        NULL  \n",
       "Chaharmahal & bakhtiari        True   Chaharmahal and Bakhtiari  \n",
       "Esfahan                        True                     Isfahan  \n",
       "Khorasan Jonobi               False                        NULL  \n",
       "Khorasan Razavi               False                        NULL  \n",
       "Khorasan Shomali              False                        NULL  \n",
       "Kohgiluyeh & Boyerahmad        True  Kohgiluyeh and Boyer-Ahmad  \n",
       "Kordestan                     False                        NULL  \n",
       "Sistan & Bluchestan            True      Sistan and Baluchestan  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Province Matching for human data\n",
    "provs1 = human_data.loc[human_data['Province'] != 'Null']['Province']\n",
    "provs2 = iran_data['province_en']\n",
    "\n",
    "likely_matches(provs1, provs2, caps = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'Azarbaijan Gharbi',\n",
    "    'East Azerbaijan':'Azarbaijan Sharghi',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh & Boyerahmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan'\n",
    "              }\n",
    "\n",
    "## Invert dictionary - want keys to be from the human data\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "human_data['Province'] = human_data['Province'].map(match_dict_prov).fillna(human_data['Province'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of matching counties was more laborious since there were far more counties that needed to be matched, which made it difficult to confirm matches through visual inspection. Still, the likely_matches function reduced the discrepant name pairs from more than 200 to 40 that required manual matching. After populating a dictionary with the automatched names and the manually matched names, we updated the county names in the human data to be consistent with the shapefile county names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County Matching\n",
    "## Remove null values for matching\n",
    "counties1 = human_data.loc[human_data['County'] != 'Null']['County']\n",
    "counties2 = iran_data['county_en']\n",
    "\n",
    "## Create mapping of likely pairs\n",
    "matched_df = likely_matches(counties1, counties2)\n",
    "#match_names(counties1, counties2, as_df = False)\n",
    "\n",
    "#matched_df[matched_df['matched'] == 'NULL']\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "\n",
    "## Revert automatched names back to original form data files and zip matched pairs into dictionary\n",
    "caps_mappings1 = map_caps(counties1)\n",
    "caps_mappings2 = map_caps(counties2)\n",
    "\n",
    "match_dict_cty = dict(zip(automatched.index.map(caps_mappings1), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manual matching\n",
    "match_dict_man = {\n",
    "    'Ali Abad Katul':'Aliabad', \n",
    "    'Bafgh':'Bafq', \n",
    "    'Bandar Qaz':'Bandar-e-Gaz', \n",
    "    'Dailam':'Deylam',\n",
    "    'Gonbad  kavoos':'Gonbad-e-Kavus', \n",
    "    'Ijroud':'Eejrud', \n",
    "    'Jovein':'Jowayin',\n",
    "    'Kalale':'Kolaleh',\n",
    "    'Mahvalat':'Mahvelat', \n",
    "    'Menojan':'Manujan', ## auto-matched incorrectly\n",
    "    'Neyshabur':'Nishapur',  \n",
    "    'Orzoieyeh':'Arzuiyeh', \n",
    "    'Ray':'Rey', \n",
    "    'Tehran Jonub':'Tehran', \n",
    "    'Tehran Shomal':'Tehran', \n",
    "    'Tiran o Karvan':'Tiran-o-Korun',\n",
    "    'Abadeh Tashk':'Abadeh',\n",
    "    'Agh Ghala':'Aqqala',\n",
    "    'Ahvaz e gharb':'Ahvaz',\n",
    "    'Ahvaz e Shargh':'Ahvaz',\n",
    "    'Gilan Qarb':'Gilan-e-Gharb',\n",
    "    'Kharame':'Kherameh',\n",
    "    'Maraqe':'Maragheh',\n",
    "    'Tehran Gharb':'Tehran',\n",
    "    'Tehran Shargh':'Tehran',\n",
    "    'Tehran Shomal Qarb':'Tehran',\n",
    "    'Zaveh':'Zave',\n",
    "    'Bandar Mahshahr':'Mahshahr',\n",
    "    'Qale ganj':'Ghaleye-Ganj',\n",
    "    'Sarchahan':'Hajiabad',\n",
    "    'Kamfirouz':'Marvdasht',\n",
    "    'Zarghan': 'Shiraz',\n",
    "    'Beyza':'Sepidan',\n",
    "    'Dore Chagni':'Doureh',\n",
    "    'Sepid Dasht':'Khorramabad',\n",
    "    'Nour Abad':'Mamasani',\n",
    "    'Aleshtar':'Selseleh',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Saduq':'Yazd',\n",
    "    'Mashhad Morghab':'Khorrambid',\n",
    "    'Dehdez':'Izeh',\n",
    "    'zaboli':'Mehrestan',\n",
    "    'Qaemiyeh':'Kazerun',\n",
    "    'Samen ol Aemmeh':'Mashhad',\n",
    "    'kish':'Bandar-Lengeh'\n",
    "              }\n",
    "\n",
    "## Add manually matched names to the mapping dictionary\n",
    "match_dict_cty.update(match_dict_man) \n",
    "\n",
    "## Add all the perfect matches to this dictionary\n",
    "## Mapping dictionary now should include all matches\n",
    "perf_matches = np.intersect1d(iran_data['county_en'].str.capitalize(), human_data['County'].str.capitalize())\n",
    "match_dict_cty.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Map names in dataframe based on dictionary\n",
    "human_data['County'] = human_data['County'].map(match_dict_cty).fillna(human_data['County'])\n",
    "\n",
    "## Joining ##\n",
    "human_sp_data = pd.merge(human_data, iran_data, how = 'outer', left_on = 'County', right_on = 'county_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the animal data (and other datasets) could potentially have names inconsistent with both the human data and the shapefile, we needed to perform a similar procedure for each dataset to be merged. Following is the code to pair province names and county names for the animal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Animal data province matching\n",
    "\n",
    "#provs1 = animal_data['province']\n",
    "#provs2 = iran_data['province_en']\n",
    "\n",
    "#likely_matches(provs1, provs2, caps = False)\n",
    "#match_names(provs1, provs2, as_df = True, caps = False, unique = True)\n",
    "\n",
    "## Province matchings - this accounts for all discrepancies\n",
    "match_dict_prov = {\n",
    "    'West Azerbaijan':'West Azarbayjan',\n",
    "    'East Azerbaijan':'East Azarbayjan',\n",
    "    'Chaharmahal and Bakhtiari':'Chaharmahal & bakhtiari',\n",
    "    'Isfahan':'Esfahan',\n",
    "    'South Khorasan':'Khorasan Jonobi',\n",
    "    'North Khorasan':'Khorasan Shomali',\n",
    "    'Razavi Khorasan':'Khorasan Razavi',\n",
    "    'Sistan and Baluchestan':'Sistan & Bluchestan',\n",
    "    'Hamadan':'Hamedan',\n",
    "    'Kermanshan':'Kermanshah',\n",
    "    'Kohgiluyeh and Boyer-Ahmad':'Kohgiluyeh and BoyerAhmad',\n",
    "    'Kurdistan':'Kordestan',\n",
    "    'Kerman':'South Kerman'\n",
    "              }\n",
    "\n",
    "## Invert dictionary\n",
    "match_dict_prov = {v: k for k, v in match_dict_prov.items()}\n",
    "\n",
    "## Update province names in human data with dictionary mappings\n",
    "animal_data['province'] = animal_data['province'].map(match_dict_prov).fillna(animal_data['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## County names ##\n",
    "ani_cnties = animal_data['county']\n",
    "\n",
    "matched_df = likely_matches(ani_cnties, iran_data['county_en'])\n",
    "#match_names(ani_cnties, counties2, as_df = False)\n",
    "\n",
    "ani_caps_mappings = map_caps(ani_cnties)\n",
    "\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "#unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start mapping dictionary with the automatched names\n",
    "match_dict_ani = dict(zip(automatched.index.map(ani_caps_mappings), automatched['matched'].map(caps_mappings2)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_ani = {\n",
    "    'Aran and Bidgol':'Aran-o-Bidgol',\n",
    "    'Buin and Miandasht':'Booeino Miyandasht',\n",
    "    'Deyr':'Dayyer',\n",
    "    'Haftkel':'Haftgol',\n",
    "    'Ijrud':'Eejrud',\n",
    "    'Maneh asd Samalgan':'Maneh-o-Samalqan',\n",
    "    'Orzueeyeh':'Arzuiyeh',\n",
    "    'Qaleh Ganj':'Ghaleye-Ganj',\n",
    "    'Qir and Karzin':'Qir-o-Karzin',\n",
    "    'Raz and Jargalan':'Razo Jalgelan',\n",
    "    'Sib and Suran':'Sibo Soran',\n",
    "    'Tiran and Karvan':'Tiran-o-Korun',\n",
    "    'Torqebeh and Shandiz(Binalud)':'Torghabe-o-Shandiz',\n",
    "    'Zaveh':'Zave',\n",
    "    'Chardavol':'Shirvan-o-Chardavol',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'mahshahr':'Mahshahr',\n",
    "    'Jafarieh':'Torbat-e-Jam',\n",
    "    'Kahak':'Sabzevar',\n",
    "    'Kohgiluyeh and BoyerAhmad':'Kohgeluyeh',\n",
    "    'BoyerAhmad':'Yasooj'\n",
    "              }\n",
    "\n",
    "match_dict_ani.update(match_dict_man_ani)\n",
    "\n",
    "#unmatched2 = [name for name in unmatched.index.map(ani_caps_mappings) if not name in match_dict_ani.keys()]\n",
    "\n",
    "## Add identical matches to the dictionary\n",
    "perf_matches = np.intersect1d(iran_data['county_en'], animal_data['county'])\n",
    "match_dict_ani.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "## Update county names in animal data and do the join\n",
    "animal_data['county'] = animal_data['county'].map(match_dict_ani).fillna(animal_data['county'])\n",
    "ani_sp_data = pd.merge(animal_data, iran_data, how = 'outer', left_on = 'county', right_on = 'county_en')\n",
    "\n",
    "## Sum infection data grouped by county, year, and month and remerge on animal data\n",
    "ani_sp_data_grp = ani_sp_data.groupby(['county', 'year', 'month'], as_index = False)[ani_sp_data.columns[10:15]].sum()\n",
    "ani_sp_data = pd.merge(ani_sp_data, ani_sp_data_grp, how = 'left')\n",
    "\n",
    "## Calculate infection rate\n",
    "ani_sp_data['animal_inf_rate'] = ani_sp_data['n_infected']/ani_sp_data['n_sample']\n",
    "\n",
    "## Write mapping dictionary to csv for ease of QA\n",
    "# pd.DataFrame.from_dict(data=match_dict_ani, orient='index').to_csv(fp + '/animal_data_mappings.csv', index_label = ['animal_county'], header = ['shp_county'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The socioeconomic status data fortunately was accounted for by the matches identified automatically by the likely_matches function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get province names\n",
    "ses_provs = ses_data['province'].unique()\n",
    "\n",
    "## Automatch ses names to spatial data province names\n",
    "ses_matches = likely_matches(ses_provs, iran_data['province_en'].unique())\n",
    "\n",
    "## Maps from matched names back to uncapitalized names\n",
    "ses_caps_map = map_caps(ses_provs)\n",
    "iran_prov_map = map_caps(iran_data['province_en'].unique())\n",
    "\n",
    "## Determine names that were successfully matched\n",
    "automatched = ses_matches[ses_matches['matched'] != 'NULL']\n",
    "\n",
    "## Create dictionary mapping ses names to spatial data names and update ses_data\n",
    "match_dict_ses = dict(zip(automatched.index.map(ses_caps_map), automatched['matched'].map(iran_prov_map)))\n",
    "ses_data['province'] = ses_data['province'].map(match_dict_ses).fillna(ses_data['province'])\n",
    "\n",
    "## Join data\n",
    "ses_sp_data = pd.merge(ses_data, iran_data, how = 'outer', left_on = 'province', right_on = 'province_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population data proceeded in a similar manner, but province names and county names were not separated in the population csv file. So, we needed to match all names simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to append provinces and counties from iran_data because the pop data is not separated by prov/county\n",
    "provs = iran_data['province_en']\n",
    "cts = iran_data['county_en']\n",
    "\n",
    "all_names = provs.append(cts)\n",
    "\n",
    "## Auto-match names\n",
    "matched_df = likely_matches(pop_data['Description'], all_names)\n",
    "\n",
    "## Map original data names to capitalized names\n",
    "pop_caps_mappings = map_caps(pop_data['Description'])\n",
    "iran_caps_mappings = map_caps(all_names)\n",
    "\n",
    "## Identify automatched and unmatched names\n",
    "automatched = matched_df[matched_df['matched'] != 'NULL']\n",
    "unmatched = matched_df[matched_df['matched'] == 'NULL']\n",
    "\n",
    "## Start match dictionary with automatched names\n",
    "match_dict_pop = dict(zip(automatched.index.map(pop_caps_mappings), automatched['matched'].map(iran_caps_mappings)))\n",
    "\n",
    "## Manually updated name mappings\n",
    "match_dict_man_pop = {\n",
    "    'Arzooeyeh':'Arzuiyeh',\n",
    "    'Bafgh':'Bafq',\n",
    "    'Boyerahmad':'Yasooj',\n",
    "    'Firooze':'Firuzeh',\n",
    "    'Ijerud':'Eejrud',\n",
    "    'Ivan':'Eyvan',\n",
    "    'Jovin':'Jowayin',\n",
    "    'Mayamee':'Meyami',\n",
    "    'Naeen':'Nain',\n",
    "    'Neemrooz':'Nimrouz',\n",
    "    'Neyshabur':'Nishapur',\n",
    "    'Torkaman':'Bandar-e-Torkaman',\n",
    "    'Zaveh':'Zave',\n",
    "    'Khorasan-e-Razavi':'Razavi Khorasan',\n",
    "    'Qaleh-Ganj':'Ghaleye-Ganj',\n",
    "    'Qaser-e Qand':'Ghasre Ghand',\n",
    "    'Raz & Jargalan':'Razo Jalgelan',\n",
    "    'Reegan':'Rigan',\n",
    "    'Savadkuh-e Shomali':'Northern Savadkooh',\n",
    "    'Sireek':'Sirik',\n",
    "    'Sumaehsara':'Some\\'e-Sara',\n",
    "    'Tiran & Karvan':'Tiran-o-Korun',\n",
    "    'Zeerkooh':'Zirkouh',\n",
    "    'Bandar-e-Mahshahr':'Mahshahr',\n",
    "    'Bon':'Ben',\n",
    "    'Chardavel':'Shirvan-o-Chardavol',\n",
    "    'Fonuch':'Fanouj',\n",
    "    'Keyar':'Kiaar',\n",
    "    'Qayenat':'Qaen',\n",
    "    'Qods':'Shahr-e Qods',\n",
    "    'Sibsavaran':'Sibo Soran',\n",
    "    'Kordestan':'Kurdistan',\n",
    "    'Binalood':'Torghabe-o-Shandiz',\n",
    "    'Nayer':'Nir'\n",
    "              }\n",
    "\n",
    "## Update dictionary with manual matches\n",
    "match_dict_pop.update(match_dict_man_pop)\n",
    "\n",
    "## Still unmatched:\n",
    "#unmatched2 = [name for name in unmatched.index.map(pop_caps_mappings) if not name in match_dict_pop.keys()]\n",
    "\n",
    "## Add identical matches to dictionary\n",
    "perf_matches = np.intersect1d(all_names, pop_data['Description'])\n",
    "match_dict_pop.update(dict(zip(perf_matches, perf_matches)))\n",
    "\n",
    "pop_data['Mapped'] = pop_data['Description'].map(match_dict_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are cases where a county has the same name as its province, so we also had to identify which entries were associated with provinces and which were associated with counties even after successfully matching names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnroberts/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "## Disentangling instances where provinces and county names match (since they're all lumped together in this case):\n",
    "## Identify duplicate names\n",
    "dup_names = pop_data[pd.DataFrame.duplicated(pop_data, 'Description')][['Description','Mapped']]\n",
    "\n",
    "## Sort by population. We know that the first entry for each name will be the province pop\n",
    "## and the second entry will be the county prop since prov pop >= county pop\n",
    "dup_vals = pop_data[pop_data['Description'].isin(dup_names['Description'])].sort_values(by = ['Description','Population'], ascending=[True, False])\n",
    "dup_vals['Geog_region'] = np.resize(['Province','County'], len(dup_vals))\n",
    "\n",
    "## Tagging each name as either province or county\n",
    "dup_pop_data_provs = dup_vals[dup_vals['Geog_region'] == 'Province']\n",
    "nondup_pop_data_provs = pop_data[pop_data['Mapped'].isin(provs.sort_values().unique()) & (~pop_data['Description'].isin(dup_pop_data_provs['Description']))]\n",
    "nondup_pop_data_provs['Geog_region'] = 'Province'\n",
    "\n",
    "## Merge back on population data - now everything is tagged to indicate province or county\n",
    "merge1 = pd.merge(nondup_pop_data_provs, dup_vals, how='outer')\n",
    "merge2 = pd.merge(merge1, pop_data, how='outer')\n",
    "\n",
    "## Drop provinces - we only want counties\n",
    "pop_data_cts_only = merge2[merge2['Geog_region']!='Province'][['Mapped','Population']]\n",
    "\n",
    "## Add independently gathered data for counties not present in the population data \n",
    "missing_vals = pd.DataFrame([['Urumia', 736224], ['Khusf', 24922]], columns = list(['Mapped', 'Population']))\n",
    "pop_data_cts_only = pop_data_cts_only.append(missing_vals).reset_index(drop = True)\n",
    "\n",
    "## Merge with spatial data on county name\n",
    "pop_sp_data = pd.merge(pop_data_cts_only, iran_data, how = 'outer', left_on = 'Mapped', right_on = 'county_en')\n",
    "\n",
    "## Drop erroneous row - results from original pop_data file having this entry listed twice.\n",
    "pop_sp_data = pop_sp_data[pop_sp_data['Mapped']!='Razavi Khorasan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mapped</th>\n",
       "      <th>Population</th>\n",
       "      <th>county_en</th>\n",
       "      <th>county_fa</th>\n",
       "      <th>province_en</th>\n",
       "      <th>province_fa</th>\n",
       "      <th>shape_len</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alborz</td>\n",
       "      <td>242865</td>\n",
       "      <td>Alborz</td>\n",
       "      <td>شهرستان البرز</td>\n",
       "      <td>Qazvin</td>\n",
       "      <td>قزوین</td>\n",
       "      <td>1.495337</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>POLYGON ((50.00181 36.22459, 50.04015 36.22283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ardabil</td>\n",
       "      <td>605992</td>\n",
       "      <td>Ardabil</td>\n",
       "      <td>شهرستان اردبیل</td>\n",
       "      <td>Ardabil</td>\n",
       "      <td>اردبیل</td>\n",
       "      <td>3.392549</td>\n",
       "      <td>0.223132</td>\n",
       "      <td>POLYGON ((47.83857 38.26663, 47.84359 38.26782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bushehr</td>\n",
       "      <td>298594</td>\n",
       "      <td>Bushehr</td>\n",
       "      <td>شهرستان بوشهر</td>\n",
       "      <td>Bushehr</td>\n",
       "      <td>بوشهر</td>\n",
       "      <td>4.180251</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>MULTIPOLYGON (((50.65475 29.14808, 50.66041 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isfahan</td>\n",
       "      <td>2243249</td>\n",
       "      <td>Isfahan</td>\n",
       "      <td>شهرستان اصفهان</td>\n",
       "      <td>Isfahan</td>\n",
       "      <td>اصفهان</td>\n",
       "      <td>6.478881</td>\n",
       "      <td>1.502590</td>\n",
       "      <td>POLYGON ((52.30714 33.01380, 52.30826 33.01352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hamadan</td>\n",
       "      <td>676105</td>\n",
       "      <td>Hamadan</td>\n",
       "      <td>شهرستان همدان</td>\n",
       "      <td>Hamadan</td>\n",
       "      <td>همدان</td>\n",
       "      <td>2.606478</td>\n",
       "      <td>0.273418</td>\n",
       "      <td>POLYGON ((48.35279 34.76661, 48.35585 34.77523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Khatam</td>\n",
       "      <td>36562</td>\n",
       "      <td>Khatam</td>\n",
       "      <td>شهرستان خاتم</td>\n",
       "      <td>Yazd</td>\n",
       "      <td>یزد</td>\n",
       "      <td>4.118767</td>\n",
       "      <td>0.767438</td>\n",
       "      <td>POLYGON ((53.83888 30.65681, 53.83944 30.66070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Mehriz</td>\n",
       "      <td>51733</td>\n",
       "      <td>Mehriz</td>\n",
       "      <td>شهرستان مهریز</td>\n",
       "      <td>Yazd</td>\n",
       "      <td>یزد</td>\n",
       "      <td>3.619152</td>\n",
       "      <td>0.637547</td>\n",
       "      <td>POLYGON ((54.12389 31.33070, 54.12417 31.33209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Meybod</td>\n",
       "      <td>99727</td>\n",
       "      <td>Meybod</td>\n",
       "      <td>شهرستان میبد</td>\n",
       "      <td>Yazd</td>\n",
       "      <td>یزد</td>\n",
       "      <td>3.715907</td>\n",
       "      <td>0.474304</td>\n",
       "      <td>POLYGON ((54.39557 32.32517, 54.39690 32.32333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Urumia</td>\n",
       "      <td>736224</td>\n",
       "      <td>Urumia</td>\n",
       "      <td>شهرستان ارومیه</td>\n",
       "      <td>West Azerbaijan</td>\n",
       "      <td>آذربایجان غربی</td>\n",
       "      <td>4.847633</td>\n",
       "      <td>0.539024</td>\n",
       "      <td>POLYGON ((44.39222 37.82788, 44.39242 37.82847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Khusf</td>\n",
       "      <td>24922</td>\n",
       "      <td>Khusf</td>\n",
       "      <td>شهرستان خوسف</td>\n",
       "      <td>South Khorasan</td>\n",
       "      <td>خراسان جنوبی</td>\n",
       "      <td>5.896399</td>\n",
       "      <td>1.499065</td>\n",
       "      <td>POLYGON ((57.99435 31.69405, 57.99442 31.69417...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Mapped  Population county_en       county_fa      province_en  \\\n",
       "0     Alborz      242865    Alborz   شهرستان البرز           Qazvin   \n",
       "1    Ardabil      605992   Ardabil  شهرستان اردبیل          Ardabil   \n",
       "2    Bushehr      298594   Bushehr   شهرستان بوشهر          Bushehr   \n",
       "3    Isfahan     2243249   Isfahan  شهرستان اصفهان          Isfahan   \n",
       "4    Hamadan      676105   Hamadan   شهرستان همدان          Hamadan   \n",
       "..       ...         ...       ...             ...              ...   \n",
       "428   Khatam       36562    Khatam    شهرستان خاتم             Yazd   \n",
       "429   Mehriz       51733    Mehriz   شهرستان مهریز             Yazd   \n",
       "430   Meybod       99727    Meybod    شهرستان میبد             Yazd   \n",
       "431   Urumia      736224    Urumia  شهرستان ارومیه  West Azerbaijan   \n",
       "432    Khusf       24922     Khusf    شهرستان خوسف   South Khorasan   \n",
       "\n",
       "        province_fa  shape_len  shape_area  \\\n",
       "0             قزوین   1.495337    0.039376   \n",
       "1            اردبیل   3.392549    0.223132   \n",
       "2             بوشهر   4.180251    0.121400   \n",
       "3            اصفهان   6.478881    1.502590   \n",
       "4             همدان   2.606478    0.273418   \n",
       "..              ...        ...         ...   \n",
       "428             یزد   4.118767    0.767438   \n",
       "429             یزد   3.619152    0.637547   \n",
       "430             یزد   3.715907    0.474304   \n",
       "431  آذربایجان غربی   4.847633    0.539024   \n",
       "432    خراسان جنوبی   5.896399    1.499065   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((50.00181 36.22459, 50.04015 36.22283...  \n",
       "1    POLYGON ((47.83857 38.26663, 47.84359 38.26782...  \n",
       "2    MULTIPOLYGON (((50.65475 29.14808, 50.66041 29...  \n",
       "3    POLYGON ((52.30714 33.01380, 52.30826 33.01352...  \n",
       "4    POLYGON ((48.35279 34.76661, 48.35585 34.77523...  \n",
       "..                                                 ...  \n",
       "428  POLYGON ((53.83888 30.65681, 53.83944 30.66070...  \n",
       "429  POLYGON ((54.12389 31.33070, 54.12417 31.33209...  \n",
       "430  POLYGON ((54.39557 32.32517, 54.39690 32.32333...  \n",
       "431  POLYGON ((44.39222 37.82788, 44.39242 37.82847...  \n",
       "432  POLYGON ((57.99435 31.69405, 57.99442 31.69417...  \n",
       "\n",
       "[432 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_sp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the environmental dataset already had the same province and county names as the iran shapefile, it did not need to be cleaned. So, at this point, we finally had all our data merged onto the iran shapefile data for spatial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aggregation \n",
    "\n",
    "In this stage, the previously refined data will be aggregated to reach a single number representing a total number of disease cases for each county. The output of this step would be a geo data frame holding a name, population, total disease cases, and a geometry per county. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the preprocessed data\n",
    "df = human_sp_data\n",
    "df_pop = pop_sp_data\n",
    "cum = 0\n",
    "coun = []\n",
    "agg = []\n",
    "geom = []\n",
    "pop = []\n",
    "j = 0\n",
    "\n",
    "for cty in df['county_en'].unique(): # Looping over county names\n",
    "    for i in range(len(df)): # Looping over data rows\n",
    "        if df.iloc[i, -7] == cty: # checking to see if the county name for a patient record matches the county name in the current loop\n",
    "            if cum == 0: # Taking the county name only once \n",
    "                coun.append(df.iloc[i, -7]) \n",
    "                for p in range(len(df_pop)): #Looping over the other population data \n",
    "                    if df_pop.iloc[p, 2] == cty: # checking to see if the county names match\n",
    "                        pop.append(df_pop.iloc[p, 1]) # getting the population value for each county \n",
    "                        geom.append(df_pop.iloc[p, -1]) # taking the geometry from pop data frame\n",
    "                        \n",
    "            cum += 1 # summing up the number of patients per county\n",
    "    \n",
    "    agg.append(cum) # Appending the disease population for each county\n",
    "    cum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a pandas data frame for derived columns\n",
    "ag_data = pd.DataFrame([coun, pop, agg, geom]).T\n",
    "headers = ['county_en', 'population', 'bruc', 'geometry']\n",
    "ag_data.columns = headers\n",
    "\n",
    "# Deleting the rows having NaN geometries due to table inconsistencies/data duplications\n",
    "ag_data2 = ag_data.drop([429,430,431])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_en</th>\n",
       "      <th>population</th>\n",
       "      <th>bruc</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eshtehard</td>\n",
       "      <td>37876</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((50.16315170000007 35.61053360000005,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fardis</td>\n",
       "      <td>271829</td>\n",
       "      <td>85</td>\n",
       "      <td>POLYGON ((50.89902560100006 35.71893810100005,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karaj</td>\n",
       "      <td>1973470</td>\n",
       "      <td>306</td>\n",
       "      <td>POLYGON ((50.73984450100005 35.69302690000006,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nazarabad</td>\n",
       "      <td>152437</td>\n",
       "      <td>143</td>\n",
       "      <td>POLYGON ((50.33135260000006 35.85733750000003,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Savojbolagh</td>\n",
       "      <td>259973</td>\n",
       "      <td>151</td>\n",
       "      <td>POLYGON ((50.56122609900007 36.04331280000002,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Khamir</td>\n",
       "      <td>143973</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((47.42330560000005 34.06604070100008,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Khorobiyabanak</td>\n",
       "      <td>107801</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((52.00064940000004 27.84925220000002,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Meyami</td>\n",
       "      <td>23648</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((50.91818670000004 36.27082500000006,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Northern Savadkooh</td>\n",
       "      <td>56148</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((54.89503560000003 27.28061970000005,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Razo Jalgelan</td>\n",
       "      <td>19761</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((54.16834130000007 34.01465400000006,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              county_en population bruc  \\\n",
       "0             Eshtehard      37876    5   \n",
       "1                Fardis     271829   85   \n",
       "2                 Karaj    1973470  306   \n",
       "3             Nazarabad     152437  143   \n",
       "4           Savojbolagh     259973  151   \n",
       "..                  ...        ...  ...   \n",
       "424              Khamir     143973    1   \n",
       "425      Khorobiyabanak     107801    1   \n",
       "426              Meyami      23648    1   \n",
       "427  Northern Savadkooh      56148    1   \n",
       "428       Razo Jalgelan      19761    1   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((50.16315170000007 35.61053360000005,...  \n",
       "1    POLYGON ((50.89902560100006 35.71893810100005,...  \n",
       "2    POLYGON ((50.73984450100005 35.69302690000006,...  \n",
       "3    POLYGON ((50.33135260000006 35.85733750000003,...  \n",
       "4    POLYGON ((50.56122609900007 36.04331280000002,...  \n",
       "..                                                 ...  \n",
       "424  POLYGON ((47.42330560000005 34.06604070100008,...  \n",
       "425  POLYGON ((52.00064940000004 27.84925220000002,...  \n",
       "426  POLYGON ((50.91818670000004 36.27082500000006,...  \n",
       "427  POLYGON ((54.89503560000003 27.28061970000005,...  \n",
       "428  POLYGON ((54.16834130000007 34.01465400000006,...  \n",
       "\n",
       "[429 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation and Hotspot Detection \n",
    "\n",
    "### Global Moran's I\n",
    "\n",
    "To investigate the type of spatial pattern (i.e., random, uniform, and clustered) of the disease incidence in Iran, global Moran’s I analysis is employed. The Global Moran’s I measures the spatial autocorrelation of the cities. \n",
    "The expected output of this step will be an index ranging between -1 and 1, where -1 represents a dispersed distribution, 0, a random and 1 shows a clustered distribution status. \n",
    "\n",
    "### Spatial Weights Matrix (w)\n",
    "\n",
    "For this research, the Queen neighbourhood strategy is considered. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WARNING: ', 168, ' is an island (no neighbors)')\n",
      "('WARNING: ', 419, ' is an island (no neighbors)')\n",
      "('WARNING: ', 168, ' is an island (no neighbors)')\n",
      "('WARNING: ', 419, ' is an island (no neighbors)')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('float64'), dtype('O'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f99a35b69ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The p-value is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmoran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mGlobal_morans_I\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-f99a35b69ec2>\u001b[0m in \u001b[0;36mGlobal_morans_I\u001b[0;34m(gdf)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmoran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoran\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Calling Moran's I function and passing the value y and the neighbourhood matrix w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The global Moran\\'s index is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, y, w, transformation, permutations, two_tailed)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEI\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseI_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEI\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseI_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36m__calc\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mzl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0minum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2ss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/libpysal/weights/spatial_lag.py\u001b[0m in \u001b[0;36mlag_spatial\u001b[0;34m(w, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;31m# Fast path for the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# output array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         result = np.zeros(M, dtype=upcast_char(self.dtype.char,\n\u001b[0;32m--> 464\u001b[0;31m                                                other.dtype.char))\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast_char\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_upcast_memo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('float64'), dtype('O'))"
     ]
    }
   ],
   "source": [
    "def Global_morans_I(gdf):\n",
    "    y = gdf['bruc'].values # Choosing the value of interest. Here: the total number of disease cases for each county. \n",
    "    # Creating a Queen based contiguity weights matrix\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "    w.transform = 'r'\n",
    "    \n",
    "\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "    moran = Moran(y, w) #Calling Moran's I function and passing the value y and the neighbourhood matrix w\n",
    "    print('The global Moran\\'s index is', moran.I)\n",
    "    \n",
    "    fig, ax = moran_scatterplot(moran, aspect_equal=True) # Calling the function and passing the calculated Moran's I\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    #Plotting Moran's I diagram for our case to see how significant is our positive autocorrelation. \n",
    "    plot_moran(moran, zstandard=True, figsize=(10,4))\n",
    "    plt.show()\n",
    "    \n",
    "    #Plotting the p-value of the result. p-value is a statistical measure of significance. \n",
    "    # any p-value smaller than 0.05 would be considerred statistically significant for the level of at least %95. \n",
    "    print('The p-value is',moran.p_sim)\n",
    "\n",
    "Global_morans_I(ag_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the global Moran's index is significantly bigger than zero we could say the data is clustered. \n",
    "From the positive sign of the index we could infer that there is a positive association among the polygons according to the total number of disease cases per county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WARNING: ', 168, ' is an island (no neighbors)')\n",
      "('WARNING: ', 419, ' is an island (no neighbors)')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('float64'), dtype('O'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-cc96fff99533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mLocal_morans_I\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-cc96fff99533>\u001b[0m in \u001b[0;36mLocal_morans_I\u001b[0;34m(gdf)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# calculate Local Moran's I to detect the disease hot and cold spots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmoran_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoran_Local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoran_scatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoran_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Disease Total Incidence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, y, w, transformation, permutations, geoda_quads)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoda_quads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeoda_quads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mquads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, w, z)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mzl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/libpysal/weights/spatial_lag.py\u001b[0m in \u001b[0;36mlag_spatial\u001b[0;34m(w, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;31m# Fast path for the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# output array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         result = np.zeros(M, dtype=upcast_char(self.dtype.char,\n\u001b[0;32m--> 464\u001b[0;31m                                                other.dtype.char))\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast_char\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_upcast_memo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('float64'), dtype('O'))"
     ]
    }
   ],
   "source": [
    "def Local_morans_I(gdf):\n",
    "    y = gdf['bruc'].values\n",
    "    w = Queen.from_dataframe(gdf)\n",
    "    w.transform = 'r'\n",
    "\n",
    "# calculate Local Moran's I to detect the disease hot and cold spots\n",
    "    moran_loc = Moran_Local(y, w) \n",
    "    fig, ax = moran_scatterplot(moran_loc)\n",
    "    ax.set_xlabel('Disease Total Incidence')\n",
    "    ax.set_ylabel('Spatial Lag of Incidences')\n",
    "    plt.show()\n",
    "    \n",
    "    # Running Moran's scatterplot function for the computed local Moran's I. \n",
    "    fig, ax = moran_scatterplot(moran_loc, p=0.05)\n",
    "    ax.set_xlabel('Disease Total Incidence')\n",
    "    ax.set_ylabel('Spatial Lag of Incidences')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# plotting calculated local Moran's I map. \n",
    "    lisa_cluster(moran_loc, gdf, p=0.05, figsize = (9,9))\n",
    "    plt.show()\n",
    "    \n",
    "    # plotting local Moran's I result and the scatter plot for a better interpretation.\n",
    "    \n",
    "    plot_local_autocorrelation(moran_loc, gdf, 'total')\n",
    "    plt.show()\n",
    "    \n",
    "Local_morans_I(ag_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WARNING: ', 168, ' is an island (no neighbors)')\n",
      "('WARNING: ', 419, ' is an island (no neighbors)')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('float64'), dtype('O'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7080ffd77914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmoran_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoran_Local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, y, w, transformation, permutations, geoda_quads)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoda_quads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeoda_quads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mquads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/esda/moran.py\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, w, z)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mzl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/libpysal/weights/spatial_lag.py\u001b[0m in \u001b[0;36mlag_spatial\u001b[0;34m(w, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;31m# Fast path for the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# output array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         result = np.zeros(M, dtype=upcast_char(self.dtype.char,\n\u001b[0;32m--> 464\u001b[0;31m                                                other.dtype.char))\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast_char\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_upcast_memo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('float64'), dtype('O'))"
     ]
    }
   ],
   "source": [
    "y = ag_data2['bruc'].values\n",
    "w = Queen.from_dataframe(ag_data2)\n",
    "w.transform = 'r'\n",
    "\n",
    "moran_loc = Moran_Local(y, w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results of local Moran's I analysis, there are two major disease hotspot with a statistical significance of 95%. One of the disease clusters is located in northeastern parts of the country and the second one is situate in the northwest. \n",
    "This analysis also revealed Low-Low clusters which are the significant coldspots of the disease throughout the country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding environmental Data\n",
    "The environmental data is stored in a separate csv after being pulled from Google Earth Engine. The data is read in as a pandas  dataframe indexed by county with columns for each environmental variable for each month from 1996-2018. The data is aggregated as mean value by month.\n",
    "\n",
    "The data was aggregated using the iran shapefile, so the county names already match those in the human and animal dataframes and do not need to be changed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: addGregorian\n",
    "The animal data contains only the jalali calendar date in the form of a jalali month and jalali year column. This function takes the dataframe, the year column name and the month column name and edits the dataframe in-place to add a gregorian year and month column. It utilizes the jdatetime module which easily converts jalali dates to gregorian.\n",
    "\n",
    "#### pandas.apply\n",
    ".apply() iterates a function over each row in a dataframe if axis=1 is specified and returns a series. In this case, it is used to create a new series which will be the year or month column.\n",
    "\n",
    "#### lambda functions\n",
    "Lambda functions are anonymous, meaning they are not given a name. In this case, they are useful because we need only a simple function for a short period of time. In the case of the first lambda below, it could also be written as:\n",
    "\n",
    "def myfun(row):\n",
    "\n",
    "    if (row[yearCol].isnumeric() and row[moCol].isnumeric()):\n",
    "        jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian().strftime('%y')\n",
    "    else:\n",
    "        None\n",
    "\n",
    ".apply() applies this function to each row to generate the gregorian year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdatetime\n",
    "\n",
    "def addGregorian(data, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Adds gregorian month and year columns to a dataframe based on jalali month and year columns. \n",
    "    Assumes first day of jalali month, since day is not given.\n",
    "    \"\"\"\n",
    "    #The apply function iterates a function over each row in a dataframe (axis=1 specifies row not column)\n",
    "    #The function creates a jalali date object for each row from the jalali year and month. \n",
    "    #Then, it converts it to a gregorian date and extracts the year or month. \n",
    "    #If the jalali year and date are not numeric (e.g. Null as a string) then the year and month get None values. \n",
    "    data['year']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian()\\\n",
    "                            .strftime('%y') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)\n",
    "    data['month']=data.apply(lambda row: jdatetime.date(int(row[yearCol]), int(row[moCol]), 1).togregorian()\\\n",
    "                             .strftime('%m') if (row[yearCol].isnumeric() and row[moCol].isnumeric()) else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: addEnvData\n",
    "The environmental dataframe is indexed by county and has column names following the \"variableName_YYYYMM\" format. The following are some column names in the dataframe: \n",
    "\n",
    "mean_2m_air_temperature_200801\n",
    "\n",
    "mean_total_precipitation_201008\n",
    "\n",
    "mean_ndvi_201811\n",
    "\n",
    "Because elevation doesn't typically change with time, there is only one elevation column named mean_elevation.\n",
    "\n",
    "#### .apply() and lambda functions\n",
    "Just like addGregorian(), addEnvData() uses apply and lambda functions to extract data from the environmental dataframe.\n",
    "#### row.County\n",
    "row.County is used to get the county name of the row, and to locate the corresponding row in the environmental dataframe.\n",
    "#### Appending YYYYMM to end of column name\n",
    "For each row, the _YYYYMM sequence is added to the end of the variable name to choose data from the correct column in the environmental dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEnvData(data, envDF, yearCol, moCol):\n",
    "    \"\"\"\n",
    "    Takes two dataframes: \n",
    "    data-  a dataframe indexed by county, contains at least a year and month column\n",
    "    envDF- a dataframe indexed by county, contains  columns for each environmental variable \n",
    "           for each month from 1996-2018. The data is aggregated as mean by month. \n",
    "    \n",
    "    The function pulls the corresponding county/date value for each environmental data variable\n",
    "    and adds them as a new column in 'data'.\n",
    "    \"\"\"\n",
    "    #The apply function iterates a function over each row in a dataframe (axis=1 specifies row)\n",
    "    #envDF.loc[row.County] sorts the environmental data to the correct county. \n",
    "    #The next bracket selects a column by creating a datestring in the correct format. .zfill is required to make sure months are in '04' format instead of '4'\n",
    "    data['mean_2m_air_temperature'] = data.apply(lambda row: envDF.loc[row.County]['mean_2m_air_temperature_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_total_precipitation'] = data.apply(lambda row: envDF.loc[row.County]['total_precipitation_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    data['mean_ndvi'] = data.apply(lambda row: envDF.loc[row.County]['mean_20'+row[yearCol]+row[moCol].zfill(2)] if (row[yearCol]!=None and row.County in envDF.index) else None, axis=1)\n",
    "    #Elevation doesn't change with time, so the date selector is not necessary\n",
    "    data['mean_elevation'] = data.apply(lambda row: envDF.loc[row.County]['mean_elevation'] if (row.County in envDF.index) else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human data needs to be cleaned just a little bit. Some float-type NaN values need to be converted to 'Null' strings, otherwise the functions above won't function.\n",
    "\n",
    "For the animal data, the 'county' column just needs to be capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Environmental Data\n",
    "envFP=os.path.join(fp, 'Data', 'allParams.csv')\n",
    "envData = pd.read_csv(envFP, index_col='ADM2_EN')\n",
    "\n",
    "## Clean up human data, add date column\n",
    "human_sp_data.loc[pd.isna(human_sp_data['Outbreak_yr']), 'Outbreak_yr']='Null'\n",
    "addGregorian(human_sp_data, 'Outbreak_yr', 'Outbreak_mth')\n",
    "\n",
    "## Clean up animal data\n",
    "ani_sp_data.rename(columns={\"county\": \"County\"}, inplace=True)\n",
    "\n",
    "## Adding environmental data to both dataframes\n",
    "addEnvData(ani_sp_data, envData, 'year', 'month')\n",
    "addEnvData(human_sp_data, envData, 'year', 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Future Steps\n",
    "\n",
    "- The steps ahead of this project would be: \n",
    "    * Incorporating animal vaccination \n",
    "    - Assessing possible correlation between animal and human data\n",
    "    - implementing a multiple linear regression analysis incorporating environmental variables and the standard disease incidence per 100000 people per county. \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
